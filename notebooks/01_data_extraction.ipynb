{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e9e81b",
   "metadata": {},
   "source": [
    "# Multi-Disease Data Extraction Pipeline\n",
    "\n",
    "Extract disease cases and matched controls from AGP dataset.\n",
    "\n",
    "## Summary\n",
    "- Extract cases and matched controls for a selected disease\n",
    "- Build BIOM tables and phylogenetic distance matrices\n",
    "- Save outputs under `{DISEASE}_analysis_output/`\n",
    "\n",
    "## Supported Diseases\n",
    "IBD, Diabetes, Cancer, Autoimmune, Depression, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-ibd-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DISEASE from experiment config or environment\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "EXPERIMENT_CONFIG = {}\n",
    "if 'EXPERIMENT_CONFIG_PATH' in os.environ:\n",
    "    config_path = os.environ['EXPERIMENT_CONFIG_PATH']\n",
    "    if Path(config_path).exists():\n",
    "        with open(config_path, 'r') as f:\n",
    "            EXPERIMENT_CONFIG = yaml.safe_load(f)\n",
    "\n",
    "# Extract disease from config (check multiple paths like other notebooks)\n",
    "_disease = (\n",
    "    EXPERIMENT_CONFIG.get('disease') or\n",
    "    EXPERIMENT_CONFIG.get('data_extraction', {}).get('disease') or\n",
    "    EXPERIMENT_CONFIG.get('data_extraction', {}).get('disease_criteria', {}).get('disease') or\n",
    "    os.environ.get('DISEASE', 'IBD')\n",
    ")\n",
    "DISEASE = _disease.upper()\n",
    "\n",
    "# Disease-specific configurations\n",
    "DISEASE_CONFIGS = {\n",
    "    'IBD': {\n",
    "        'column': 'ibd',\n",
    "        'positive_values': ['Diagnosed by a medical professional (doctor, physician assistant)'],\n",
    "        'negative_values': ['I do not have this condition'],\n",
    "        'type_column': 'ibd_diagnosis_refined',\n",
    "        'valid_types': [\"Crohn's disease\", \"Ulcerative colitis\"]\n",
    "    },\n",
    "    'DIABETES': {\n",
    "        'column': 'diabetes',\n",
    "        'positive_values': ['Diagnosed by a medical professional (doctor, physician assistant)'],\n",
    "        'negative_values': ['I do not have this condition'],\n",
    "        'type_column': None,\n",
    "        'valid_types': None\n",
    "    },\n",
    "    'CANCER': {\n",
    "        'column': 'cancer',\n",
    "        'positive_values': ['Diagnosed by a medical professional (doctor, physician assistant)'],\n",
    "        'negative_values': ['I do not have this condition'],\n",
    "        'type_column': 'cancer_type',\n",
    "        'valid_types': None\n",
    "    },\n",
    "    'AUTOIMMUNE': {\n",
    "        'column': 'autoimmune',\n",
    "        'positive_values': ['Diagnosed by a medical professional (doctor, physician assistant)'],\n",
    "        'negative_values': ['I do not have this condition'],\n",
    "        'type_column': None,\n",
    "        'valid_types': None\n",
    "    },\n",
    "    'DEPRESSION': {\n",
    "        'column': 'depression_bipolar_schizophrenia',\n",
    "        'positive_values': ['Diagnosed by a medical professional (doctor, physician assistant)'],\n",
    "        'negative_values': ['I do not have this condition'],\n",
    "        'type_column': None,\n",
    "        'valid_types': None\n",
    "    },\n",
    "    'MENTAL_ILLNESS': {\n",
    "        'column': 'mental_illness',\n",
    "        'positive_values': ['Yes'],\n",
    "        'negative_values': ['No'],\n",
    "        'type_column': 'mental_illness_type',\n",
    "        'valid_types': None\n",
    "    },\n",
    "    'CARDIOVASCULAR': {\n",
    "        'column': 'cardiovascular_disease',\n",
    "        'positive_values': ['Diagnosed by a medical professional (doctor, physician assistant)'],\n",
    "        'negative_values': ['I do not have this condition'],\n",
    "        'type_column': None,\n",
    "        'valid_types': None\n",
    "    },\n",
    "    'KIDNEY': {\n",
    "        'column': 'kidney_disease',\n",
    "        'positive_values': ['Diagnosed by a medical professional (doctor, physician assistant)'],\n",
    "        'negative_values': ['I do not have this condition'],\n",
    "        'type_column': None,\n",
    "        'valid_types': None\n",
    "    },\n",
    "    'LIVER': {\n",
    "        'column': 'liver_disease',\n",
    "        'positive_values': ['Diagnosed by a medical professional (doctor, physician assistant)'],\n",
    "        'negative_values': ['I do not have this condition'],\n",
    "        'type_column': None,\n",
    "        'valid_types': None\n",
    "    },\n",
    "    'LUNG': {\n",
    "        'column': 'lung_disease',\n",
    "        'positive_values': ['Diagnosed by a medical professional (doctor, physician assistant)'],\n",
    "        'negative_values': ['I do not have this condition'],\n",
    "        'type_column': None,\n",
    "        'valid_types': None\n",
    "    }\n",
    "}\n",
    "\n",
    "if DISEASE not in DISEASE_CONFIGS:\n",
    "    print(f\"WARNING: Unknown disease '{DISEASE}', using IBD defaults\")\n",
    "    DISEASE = 'IBD'\n",
    "\n",
    "DISEASE_CONFIG = DISEASE_CONFIGS[DISEASE]\n",
    "\n",
    "print(f\"{DISEASE} Data Extraction Pipeline\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Disease column: {DISEASE_CONFIG['column']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from biom import load_table\n",
    "from biom.table import Table\n",
    "import skbio\n",
    "from skbio.tree import TreeNode\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path(f\"{DISEASE}_analysis_output/checkpoint_data.pkl\")\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    print(\"Loading checkpoint...\")\n",
    "    \n",
    "    with open(checkpoint_path, 'rb') as f:\n",
    "        checkpoint_data = pickle.load(f)\n",
    "    \n",
    "    cases_valid = checkpoint_data['cases_valid']\n",
    "    matched_controls = checkpoint_data['matched_controls']\n",
    "    case_biom = checkpoint_data['case_biom']\n",
    "    control_biom = checkpoint_data['control_biom']\n",
    "    config = checkpoint_data['config']\n",
    "    biom_table = checkpoint_data['biom_table']\n",
    "    case_sample_ids = checkpoint_data['case_sample_ids']\n",
    "    control_sample_ids = checkpoint_data['control_sample_ids']\n",
    "\n",
    "    phylogeny_tip_count = checkpoint_data.get('phylogeny_tip_count')\n",
    "    distance_matrix_created = checkpoint_data.get('distance_matrix_created')\n",
    "    phylogeny_copied = checkpoint_data.get('phylogeny_copied')\n",
    "    sequences_extracted = checkpoint_data.get('sequences_extracted')\n",
    "    all_tsv_files_created = checkpoint_data.get('all_tsv_files_created')\n",
    "    \n",
    "    print(f\"Loaded: {len(cases_valid)} cases, {len(matched_controls)} controls\")\n",
    "    print(f\"{DISEASE} BIOM: {case_biom.shape if case_biom is not None else 'None'}\")\n",
    "    print(f\"Control BIOM: {control_biom.shape if control_biom is not None else 'None'}\")\n",
    "    if phylogeny_tip_count:\n",
    "        print(f\"Phylogeny tips: {phylogeny_tip_count}\")\n",
    "    print(\"Can skip to final output cells\")\n",
    "    \n",
    "    CHECKPOINT_LOADED = True\n",
    "    \n",
    "else:\n",
    "    print(\"No checkpoint - run full pipeline first\")\n",
    "    CHECKPOINT_LOADED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-etape1-phylo",
   "metadata": {},
   "source": [
    "# Multi-Disease Data Extraction\n",
    "### Steps and outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'CHECKPOINT_LOADED' in globals() and CHECKPOINT_LOADED:\n",
    "    print(\"Checkpoint loaded - skip to final cells\")\n",
    "    \n",
    "    output_dir = Path(config[\"output_dir\"]) if 'config' in globals() else Path(f\"{DISEASE}_analysis_output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for dir_name in [\"metadata\", \"biom_tables\", \"sequences\", \"phylogeny\", \"config\"]:\n",
    "        (output_dir / dir_name).mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"Output dirs ready\")\n",
    "    \n",
    "else:\n",
    "    print(\"No checkpoint - run processing cells first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phylo-read-001",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(f\"{DISEASE}_analysis_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "dirs_to_create = [\"metadata\", \"biom_tables\", \"sequences\", \"phylogeny\", \"config\"]\n",
    "\n",
    "for dir_name in dirs_to_create:\n",
    "    (output_dir / dir_name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Created dirs:\")\n",
    "for d in dirs_to_create:\n",
    "    print(f\"  {output_dir / d}\")\n",
    "\n",
    "config = {\n",
    "    \"biom_path\": \"../data/AGP.data.biom\",\n",
    "    \"metadata_path\": \"../data/AGP-metadata.tsv\", \n",
    "    \"phylogeny_path\": \"../data/2024.09.phylogeny.asv.nwk\",\n",
    "    \"sequences_path\": \"ASV_sequences.fasta\",\n",
    "    \n",
    "    \"disease\": DISEASE,\n",
    "    \"disease_column\": DISEASE_CONFIG['column'],\n",
    "    \"positive_values\": DISEASE_CONFIG['positive_values'],\n",
    "    \"negative_values\": DISEASE_CONFIG['negative_values'],\n",
    "    \"type_column\": DISEASE_CONFIG['type_column'],\n",
    "    \"valid_types\": DISEASE_CONFIG['valid_types'],\n",
    "    \n",
    "    \"age_categories\": [\"20s\", \"30s\", \"40s\", \"50s\", \"60s\", \"70+\"],\n",
    "    \"bmi_categories\": [\"Underweight\", \"Normal\", \"Overweight\", \"Obese\"],\n",
    "    \"random_seed\": 42,\n",
    "\n",
    "    \"enable_feature_filtering\": False,\n",
    "    \n",
    "    \"output_dir\": str(output_dir),\n",
    "    \"metadata_output\": str(output_dir / \"metadata\"),\n",
    "    \"biom_output\": str(output_dir / \"biom_tables\"),\n",
    "    \"sequences_output\": str(output_dir / \"sequences\"),\n",
    "    \"phylogeny_output\": str(output_dir / \"phylogeny\")\n",
    "}\n",
    "\n",
    "with open(output_dir / \"config\" / \"pipeline_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\nConfiguration saved to: {output_dir / 'config' / 'pipeline_config.json'}\")\n",
    "print(f\"Disease: {config['disease']}\")\n",
    "print(f\"Column: {config['disease_column']}\")\n",
    "print(f\"Positive values: {config['positive_values']}\")\n",
    "print(f\"Negative values: {config['negative_values']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phylo-count-tips",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_files = [\n",
    "    config[\"metadata_path\"],\n",
    "    config[\"biom_path\"], \n",
    "    config[\"phylogeny_path\"]\n",
    "]\n",
    "\n",
    "print(\"Checking required input files:\")\n",
    "missing_files = []\n",
    "for file_path in required_files:\n",
    "    if Path(file_path).exists():\n",
    "        size_mb = Path(file_path).stat().st_size / (1024*1024)\n",
    "        print(f\" {file_path} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\" {file_path} - MISSING\")\n",
    "        missing_files.append(file_path)\n",
    "\n",
    "optional_files = [config[\"sequences_path\"]]\n",
    "print(\"\\n Checking optional input files:\")\n",
    "for file_path in optional_files:\n",
    "    if Path(file_path).exists():\n",
    "        size_mb = Path(file_path).stat().st_size / (1024*1024)\n",
    "        print(f\" {file_path} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\" {file_path} - Not found (optional)\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n Missing required files: {missing_files}\")\n",
    "    print(\"Please ensure all required files are in the current directory.\")\n",
    "else:\n",
    "    print(f\"\\n All required files found!\")\n",
    "\n",
    "phylogeny_path = config[\"phylogeny_path\"]\n",
    "phylogeny_file_exists = Path(phylogeny_path).exists()\n",
    "if phylogeny_file_exists:\n",
    "    if 'phylogeny_tip_count' in globals() and phylogeny_tip_count is not None:\n",
    "        print(f\" Phylogeny file present; tips from checkpoint: {phylogeny_tip_count:,}\")\n",
    "        gg_phylogeny = None\n",
    "    else:\n",
    "        try:\n",
    "            gg_phylogeny = TreeNode.read(phylogeny_path)\n",
    "            n_tips = len(list(gg_phylogeny.tips()))\n",
    "            print(f\" Phylogeny loaded: {n_tips:,} tips\")\n",
    "            phylogeny_tip_count = n_tips\n",
    "        except Exception as e:\n",
    "            print(f\" Error loading phylogeny: {e}\")\n",
    "            gg_phylogeny = None\n",
    "            phylogeny_tip_count = None\n",
    "else:\n",
    "    print(f\" Phylogeny file missing: {phylogeny_path}\")\n",
    "    gg_phylogeny = None\n",
    "    phylogeny_tip_count = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6ad0f",
   "metadata": {},
   "source": [
    "## Load and Process Metadata\n",
    "### Read and filter metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading AGP metadata...\")\n",
    "df = pd.read_csv(config[\"metadata_path\"], sep=\"\\t\", low_memory=False, dtype=str)\n",
    "print(f\"Loaded {df.shape[0]:,} samples with {df.shape[1]} variables\")\n",
    "\n",
    "df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "sample_col = \"#SampleID\"\n",
    "disease_col = config['disease_column']\n",
    "age_col = \"age_cat\" \n",
    "bmi_col = \"bmi_cat\"\n",
    "sex_col = \"sex\"\n",
    "\n",
    "type_col = config.get('type_column')\n",
    "if type_col and type_col not in df.columns:\n",
    "    print(f\"Warning: Type column '{type_col}' not found in metadata\")\n",
    "    type_col = None\n",
    "\n",
    "print(f\"\\nKey columns identified:\")\n",
    "print(f\"  Sample ID: {sample_col}\")\n",
    "print(f\"  Disease status: {disease_col}\")\n",
    "print(f\"  Age category: {age_col}\")\n",
    "print(f\"  BMI category: {bmi_col}\")\n",
    "print(f\"  Sex: {sex_col}\")\n",
    "print(f\"  Disease type: {type_col}\")\n",
    "\n",
    "print(f\"\\nSample of metadata:\")\n",
    "display(df[[sample_col, disease_col, age_col, bmi_col, sex_col]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Identifying {DISEASE} cases and controls...\")\n",
    "\n",
    "cases = df[df[disease_col].isin(config[\"positive_values\"])].copy()\n",
    "print(f\"Total {DISEASE} cases: {len(cases):,}\")\n",
    "\n",
    "if type_col and config.get(\"valid_types\"):\n",
    "    cases = cases[cases[type_col].isin(config[\"valid_types\"])]\n",
    "    print(f\"{DISEASE} cases (valid types): {len(cases):,}\")\n",
    "    \n",
    "    if len(cases) > 0:\n",
    "        type_counts = cases[type_col].value_counts()\n",
    "        print(f\"{DISEASE} type breakdown:\")\n",
    "        for disease_type, count in type_counts.items():\n",
    "            print(f\"  - {disease_type}: {count:,}\")\n",
    "\n",
    "controls = df[df[disease_col].isin(config[\"negative_values\"])].copy()\n",
    "print(f\"Total potential controls: {len(controls):,}\")\n",
    "\n",
    "cases_valid = cases[\n",
    "    cases[age_col].isin(config[\"age_categories\"]) & \n",
    "    cases[bmi_col].isin(config[\"bmi_categories\"])\n",
    "].copy()\n",
    "\n",
    "controls_valid = controls[\n",
    "    controls[age_col].isin(config[\"age_categories\"]) & \n",
    "    controls[bmi_col].isin(config[\"bmi_categories\"])\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nAfter filtering to valid age/BMI categories:\")\n",
    "print(f\"  {DISEASE} cases: {len(cases_valid):,}\")\n",
    "print(f\"  Controls: {len(controls_valid):,}\")\n",
    "\n",
    "if len(cases_valid) > 0:\n",
    "    print(f\"\\n{DISEASE} cases age/BMI distribution:\")\n",
    "    age_bmi_dist = pd.crosstab(cases_valid[age_col], cases_valid[bmi_col])\n",
    "    display(age_bmi_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e4009",
   "metadata": {},
   "source": [
    "## Create Matched Case-Control Pairs\n",
    "### Match by age and BMI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f87fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating matched case-control pairs...\")\n",
    "\n",
    "target_counts = cases_valid.groupby([age_col, bmi_col]).size()\n",
    "print(f\"Target distribution has {len(target_counts)} age/BMI combinations\")\n",
    "\n",
    "np.random.seed(config[\"random_seed\"])\n",
    "matched_samples = []\n",
    "\n",
    "for (age_val, bmi_val), n_needed in target_counts.items():\n",
    "    matching_controls = controls_valid[\n",
    "        (controls_valid[age_col] == age_val) & \n",
    "        (controls_valid[bmi_col] == bmi_val)\n",
    "    ]\n",
    "    \n",
    "    if len(matching_controls) == 0:\n",
    "        print(f\" No controls found for age={age_val}, BMI={bmi_val}\")\n",
    "        continue\n",
    "        \n",
    "    if len(matching_controls) >= n_needed:\n",
    "        sampled = matching_controls.sample(n=n_needed, replace=False, random_state=config[\"random_seed\"])\n",
    "    else:\n",
    "        sampled = matching_controls.sample(n=n_needed, replace=True, random_state=config[\"random_seed\"])\n",
    "        print(f\" Sampling with replacement for age={age_val}, BMI={bmi_val} (needed {n_needed}, had {len(matching_controls)})\")\n",
    "    \n",
    "    matched_samples.append(sampled)\n",
    "\n",
    "if matched_samples:\n",
    "    matched_controls = pd.concat(matched_samples, ignore_index=True)\n",
    "    matched_controls = matched_controls.drop_duplicates(subset=[sample_col])\n",
    "else:\n",
    "    matched_controls = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "print(f\"\\n Matching results:\")\n",
    "print(f\"{DISEASE} cases: {len(cases_valid):,}\")\n",
    "print(f\"Matched controls: {len(matched_controls):,}\")\n",
    "\n",
    "if len(matched_controls) > 0:\n",
    "    print(f\"\\n Control age/BMI distribution:\")\n",
    "    control_dist = pd.crosstab(matched_controls[age_col], matched_controls[bmi_col])\n",
    "    display(control_dist)\n",
    "    \n",
    "    print(f\"\\n {DISEASE} case age/BMI distribution:\")\n",
    "    case_dist = pd.crosstab(cases_valid[age_col], cases_valid[bmi_col])\n",
    "    display(case_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving intermediate checkpoint (metadata processing)...\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "intermediate_checkpoint = {\n",
    "    'cases_valid': cases_valid,\n",
    "    'matched_controls': matched_controls,\n",
    "    'config': config\n",
    "}\n",
    "\n",
    "checkpoint_path = Path(config[\"output_dir\"]) / \"intermediate_checkpoint.pkl\"\n",
    "with open(checkpoint_path, 'wb') as f:\n",
    "    pickle.dump(intermediate_checkpoint, f)\n",
    "\n",
    "print(f\" Intermediate checkpoint saved to: {checkpoint_path}\")\n",
    "print(f\" Saved variables:\")\n",
    "print(f\"- cases_valid (DataFrame)\")\n",
    "print(f\"- matched_controls (DataFrame)\")  \n",
    "print(f\"- config (configuration)\")\n",
    "print(\"\\n Continue to BIOM processing...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f905b",
   "metadata": {},
   "source": [
    "## Filter BIOM Tables\n",
    "### Subset samples and filter features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and filtering BIOM table...\")\n",
    "\n",
    "biom_table = load_table(config[\"biom_path\"])\n",
    "print(f\"Complete BIOM table: {biom_table.shape[0]:,} features × {biom_table.shape[1]:,} samples\")\n",
    "\n",
    "case_sample_ids = set(cases_valid[sample_col].astype(str))\n",
    "control_sample_ids = set(matched_controls[sample_col].astype(str))\n",
    "\n",
    "print(f\"{DISEASE} samples to extract: {len(case_sample_ids):,}\")\n",
    "print(f\"Control samples to extract: {len(control_sample_ids):,}\")\n",
    "\n",
    "biom_sample_ids = set(biom_table.ids(axis='sample'))\n",
    "cases_in_biom = case_sample_ids.intersection(biom_sample_ids)\n",
    "control_in_biom = control_sample_ids.intersection(biom_sample_ids)\n",
    "\n",
    "print(f\"{DISEASE} samples found in BIOM: {len(cases_in_biom):,}\")\n",
    "print(f\"Control samples found in BIOM: {len(control_in_biom):,}\")\n",
    "\n",
    "if len(cases_in_biom) > 0:\n",
    "    case_biom = biom_table.filter(cases_in_biom, axis='sample', inplace=False)\n",
    "    print(f\"{DISEASE} BIOM table: {case_biom.shape[0]:,} features × {case_biom.shape[1]:,} samples\")\n",
    "else:\n",
    "    print(f\" No {DISEASE} samples found in BIOM table!\")\n",
    "    case_biom = None\n",
    "\n",
    "if len(control_in_biom) > 0:\n",
    "    control_biom = biom_table.filter(control_in_biom, axis='sample', inplace=False)\n",
    "    print(f\"Control BIOM table: {control_biom.shape[0]:,} features × {control_biom.shape[1]:,} samples\")\n",
    "else:\n",
    "    print(f\" No control samples found in BIOM table!\")\n",
    "    control_biom = None\n",
    "\n",
    "if config.get(\"enable_feature_filtering\", False) and (case_biom is not None or control_biom is not None):\n",
    "    print(\"\\n Applying feature filtering (T2D-style)...\")\n",
    "    min_prev = float(config.get(\"feature_min_prevalence\", 0.01))\n",
    "    min_total = float(config.get(\"feature_min_total_abundance\", 0.001))\n",
    "\n",
    "    try:\n",
    "        tables = []\n",
    "        sample_counts = 0\n",
    "        if case_biom is not None:\n",
    "            tables.append(case_biom)\n",
    "            sample_counts += case_biom.shape[1]\n",
    "        if control_biom is not None:\n",
    "            tables.append(control_biom)\n",
    "            sample_counts += control_biom.shape[1]\n",
    "        union_table = tables[0] if len(tables) == 1 else tables[0].concat(tables[1:])\n",
    "\n",
    "        nonzero_counts = union_table.matrix_data.getnnz(axis=1)\n",
    "        prevalence = nonzero_counts / sample_counts\n",
    "\n",
    "        totals = np.asarray(union_table.matrix_data.sum(axis=1)).ravel()\n",
    "\n",
    "        feature_ids = np.array(union_table.ids(axis='observation'))\n",
    "        keep_mask = (prevalence >= min_prev) & (totals >= min_total)\n",
    "        kept_features = set(feature_ids[keep_mask])\n",
    "\n",
    "        # Apply max_features safety cap\n",
    "        max_feats = int(config.get(\"max_features\", 0))\n",
    "        if max_feats > 0 and len(kept_features) > max_feats:\n",
    "            # Keep only the most prevalent features\n",
    "            kept_indices = np.where(keep_mask)[0]\n",
    "            kept_prev = prevalence[kept_indices]\n",
    "            top_k = np.argsort(-kept_prev)[:max_feats]\n",
    "            final_indices = kept_indices[top_k]\n",
    "            kept_features = set(feature_ids[final_indices])\n",
    "            print(f\"  ⚠ Capped to {max_feats:,} most prevalent features\")\n",
    "\n",
    "        print(f\" Kept {len(kept_features):,} / {len(feature_ids):,} features after filtering\")\n",
    "\n",
    "        if case_biom is not None:\n",
    "            case_biom = case_biom.filter(kept_features, axis='observation', inplace=False)\n",
    "            print(f\" {DISEASE} table now: {case_biom.shape[0]:,} features × {case_biom.shape[1]:,} samples\")\n",
    "        if control_biom is not None:\n",
    "            control_biom = control_biom.filter(kept_features, axis='observation', inplace=False)\n",
    "            print(f\" Control table now: {control_biom.shape[0]:,} features × {control_biom.shape[1]:,} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\" Feature filtering failed: {e}\")\n",
    "\n",
    "print(\"\\n Saving final checkpoint with all variables...\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "checkpoint_data = {\n",
    "    'cases_valid': cases_valid,\n",
    "    'matched_controls': matched_controls,\n",
    "    'case_biom': case_biom,\n",
    "    'control_biom': control_biom,\n",
    "    'config': config,\n",
    "    'biom_table': biom_table,\n",
    "    'case_sample_ids': case_sample_ids,\n",
    "    'control_sample_ids': control_sample_ids\n",
    "}\n",
    "\n",
    "checkpoint_path = Path(config[\"output_dir\"]) / \"checkpoint_data.pkl\"\n",
    "with open(checkpoint_path, 'wb') as f:\n",
    "    pickle.dump(checkpoint_data, f)\n",
    "\n",
    "print(f\" Final checkpoint saved to: {checkpoint_path}\")\n",
    "print(f\" Saved variables:\")\n",
    "print(f\"- cases_valid (DataFrame)\")\n",
    "print(f\"- matched_controls (DataFrame)\")  \n",
    "print(f\"- case_biom (BIOM table)\")\n",
    "print(f\"- control_biom (BIOM table)\")\n",
    "print(f\"- config (configuration)\")\n",
    "print(f\"- biom_table (original large BIOM table)\")\n",
    "print(f\"- Sample ID sets\")\n",
    "print(\"\\n If kernel crashes, restart and run the checkpoint load cell to continue from here!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c527391",
   "metadata": {},
   "source": [
    "## Sequence Extraction and Phylogeny Processing\n",
    "### Copy phylogeny and extract sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing sequences and phylogeny...\")\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "phylogeny_source = Path(config[\"phylogeny_path\"])\n",
    "phylogeny_dest = Path(config[\"phylogeny_output\"]) / f\"phylogeny_{DISEASE}.nwk\"\n",
    "\n",
    "if phylogeny_source.exists():\n",
    "    print(f\" Copying phylogeny file...\")\n",
    "    shutil.copy2(phylogeny_source, phylogeny_dest)\n",
    "    print(f\" Phylogeny copied to: {phylogeny_dest}\")\n",
    "    \n",
    "    size_mb = phylogeny_dest.stat().st_size / (1024*1024)\n",
    "    print(f\" File size: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\" Source phylogeny file not found: {phylogeny_source}\")\n",
    "\n",
    "sequences_source = Path(config[\"sequences_path\"])\n",
    "if sequences_source.exists():\n",
    "    print(f\" Processing ASV sequences...\")\n",
    "\n",
    "    obs_ids = set()\n",
    "    try:\n",
    "        if case_biom is not None:\n",
    "            obs_ids |= set(case_biom.ids(axis='observation'))\n",
    "        if control_biom is not None:\n",
    "            obs_ids |= set(control_biom.ids(axis='observation'))\n",
    "    except Exception as e:\n",
    "        print(f\" Could not collect observation IDs: {e}\")\n",
    "        obs_ids = set()\n",
    "\n",
    "    print(f\" Extracting sequences for {len(obs_ids):,} observation IDs\")\n",
    "\n",
    "    sequences_output = Path(config[\"sequences_output\"]) / f\"ASV_sequences_{DISEASE}.fasta\"\n",
    "    extracted_count = 0\n",
    "\n",
    "    with open(sequences_output, 'w') as outfile:\n",
    "        for record in SeqIO.parse(sequences_source, \"fasta\"):\n",
    "            if record.id in obs_ids:\n",
    "                SeqIO.write(record, outfile, \"fasta\")\n",
    "                extracted_count += 1\n",
    "\n",
    "    print(f\" Extracted {extracted_count:,} sequences to: {sequences_output}\")\n",
    "\n",
    "    if sequences_output.exists():\n",
    "        size_mb = sequences_output.stat().st_size / (1024*1024)\n",
    "        print(f\" Sequences file size: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\" ASV sequences file not found: {sequences_source}\")\n",
    "    print(f\" Skipping sequence extraction (optional step)\")\n",
    "\n",
    "print(f\" Sequence and phylogeny processing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518a66b",
   "metadata": {},
   "source": [
    "## Distance Matrix Generation\n",
    "### Compute distance matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating THREE phylogenetic distance matrices...\")\n",
    "print(f\"1. TreeDistMatrix - Phylogenetic tree distances\")\n",
    "print(f\"2. SeqDistMatrix - Jukes-Cantor sequence distances\")  \n",
    "print(f\"3. GraphDistMatrix - Phylogenetic network distances\")\n",
    "\n",
    "from skbio import TreeNode\n",
    "from Bio import SeqIO\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "phylogeny_path = Path(config[\"phylogeny_output\"]) / f\"phylogeny_{DISEASE}.nwk\"\n",
    "\n",
    "if phylogeny_path.exists():\n",
    "    try:\n",
    "        print(f\"\\n    Loading phylogeny from: {phylogeny_path}\")\n",
    "        tree = TreeNode.read(str(phylogeny_path))\n",
    "        total_tips = len(list(tree.tips()))\n",
    "        print(f\" Phylogeny loaded: {total_tips:,} tips\")\n",
    "\n",
    "        observed_feature_ids = set()\n",
    "        if case_biom is not None:\n",
    "            observed_feature_ids.update(case_biom.ids(axis='observation'))\n",
    "        if control_biom is not None:\n",
    "            observed_feature_ids.update(control_biom.ids(axis='observation'))\n",
    "        \n",
    "        print(f\" Using filtered BIOM feature set with {len(observed_feature_ids):,} total features\")\n",
    "\n",
    "        observed_in_tree = [tip.name for tip in tree.tips() if tip.name in observed_feature_ids]\n",
    "        print(f\" Filtering tree to {len(observed_in_tree):,} observed features...\")\n",
    "\n",
    "        MAX_TIPS_FOR_DENSE_MATRIX = 25000\n",
    "        \n",
    "        if len(observed_in_tree) == 0:\n",
    "            print(f\" No observed features found in tree - skipping distance matrix\")\n",
    "            TreeDistMatrix = None\n",
    "            SeqDistMatrix = None\n",
    "            GraphDistMatrix = None\n",
    "        else:\n",
    "            if len(observed_in_tree) > MAX_TIPS_FOR_DENSE_MATRIX:\n",
    "                print(f\" {len(observed_in_tree):,} observed tips exceed cap ({MAX_TIPS_FOR_DENSE_MATRIX}); reducing set...\")\n",
    "                observed_in_tree = observed_in_tree[:MAX_TIPS_FOR_DENSE_MATRIX]\n",
    "            \n",
    "            tree_pruned = tree.shear(observed_in_tree)\n",
    "            print(f\" Pruned tree to {len(list(tree_pruned.tips())):,} tips\")\n",
    "\n",
    "            print(f\"\\n    Computing TreeDistMatrix...\")\n",
    "            try:\n",
    "                tree_dist_mat = tree_pruned.tip_tip_distances()\n",
    "                print(f\"TreeDistMatrix: {tree_dist_mat.shape}\")\n",
    "                \n",
    "                TreeDistMatrix = {}\n",
    "                tip_names = [tip.name for tip in tree_pruned.tips()]\n",
    "                for i, tip1 in enumerate(tip_names):\n",
    "                    TreeDistMatrix[tip1] = {}\n",
    "                    for j, tip2 in enumerate(tip_names):\n",
    "                        TreeDistMatrix[tip1][tip2] = tree_dist_mat[i, j]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error computing TreeDistMatrix: {e}\")\n",
    "                TreeDistMatrix = None\n",
    "\n",
    "            print(f\"\\n    Computing SeqDistMatrix (Jukes-Cantor)...\")\n",
    "            try:\n",
    "                aln_path = Path(\"../data/T2D_seqs_CMaligned_with_Rfam.sto\")\n",
    "                if not aln_path.exists():\n",
    "                    print(f\"Alignment file not found: {aln_path}\")\n",
    "                    SeqDistMatrix = None\n",
    "                else:\n",
    "                    aln_dict = SeqIO.to_dict(SeqIO.parse(str(aln_path), \"stockholm\"))\n",
    "                    print(f\"Loaded alignment with {len(aln_dict)} sequences\")\n",
    "                    \n",
    "                    def JC_distance(seq1, seq2):\n",
    "                        \"\"\"Jukes Cantor distance: (-3/4)ln[1-p*(4/3)]\"\"\"\n",
    "                        seq1_str = str(seq1.seq).upper()\n",
    "                        seq2_str = str(seq2.seq).upper()\n",
    "                        \n",
    "                        valid_positions = 0\n",
    "                        differences = 0\n",
    "                        \n",
    "                        for i in range(min(len(seq1_str), len(seq2_str))):\n",
    "                            if seq1_str[i] not in ['-', '.', 'N'] and seq2_str[i] not in ['-', '.', 'N']:\n",
    "                                valid_positions += 1\n",
    "                                if seq1_str[i] != seq2_str[i]:\n",
    "                                    differences += 1\n",
    "                        \n",
    "                        if valid_positions == 0:\n",
    "                            return 1.0\n",
    "                        \n",
    "                        p = differences / valid_positions\n",
    "                        \n",
    "                        if p >= 0.75:\n",
    "                            return 3.0\n",
    "                        \n",
    "                        return (-3/4) * log(1 - p * (4/3))\n",
    "                    \n",
    "                    SeqDistMatrix = {}\n",
    "                    aligned_features = [f for f in observed_in_tree if f in aln_dict]\n",
    "                    print(f\"Computing distances for {len(aligned_features)} aligned features...\")\n",
    "                    \n",
    "                    for i, feat1 in enumerate(aligned_features):\n",
    "                        SeqDistMatrix[feat1] = {}\n",
    "                        for feat2 in aligned_features:\n",
    "                            SeqDistMatrix[feat1][feat2] = JC_distance(aln_dict[feat1], aln_dict[feat2])\n",
    "                        \n",
    "                        if (i + 1) % 1000 == 0:\n",
    "                            print(f\"  Progress: {i+1}/{len(aligned_features)}\")\n",
    "                    \n",
    "                    print(f\"SeqDistMatrix: {len(SeqDistMatrix)} x {len(SeqDistMatrix)} features\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error computing SeqDistMatrix: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                SeqDistMatrix = None\n",
    "\n",
    "            print(f\"\\n    Computing GraphDistMatrix (Phylogenetic Network)...\")\n",
    "            try:\n",
    "                network_path = Path(\"../data/neighbornet_txt.gml\")\n",
    "                if not network_path.exists():\n",
    "                    print(f\"Network file not found: {network_path}\")\n",
    "                    GraphDistMatrix = None\n",
    "                else:\n",
    "                    phylo_graph = nx.read_gml(str(network_path), label='id')\n",
    "                    print(f\"Loaded network with {len(phylo_graph.nodes())} nodes, {len(phylo_graph.edges())} edges\")\n",
    "                    \n",
    "                    def find_node_matching_seqID(seqID, graph):\n",
    "                        for node in graph.nodes(data=True):\n",
    "                            if \"label\" in node[1]:\n",
    "                                if node[1][\"label\"] == seqID:\n",
    "                                    return node[0]\n",
    "                        return None\n",
    "                    \n",
    "                    GraphDistMatrix = {}\n",
    "                    network_features = [f for f in observed_in_tree if find_node_matching_seqID(f, phylo_graph) is not None]\n",
    "                    print(f\"Computing distances for {len(network_features)} features in network...\")\n",
    "                    \n",
    "                    for i, feat1 in enumerate(network_features):\n",
    "                        GraphDistMatrix[feat1] = {}\n",
    "                        node1 = find_node_matching_seqID(feat1, phylo_graph)\n",
    "                        \n",
    "                        for feat2 in network_features:\n",
    "                            node2 = find_node_matching_seqID(feat2, phylo_graph)\n",
    "                            \n",
    "                            try:\n",
    "                                path = nx.shortest_path(phylo_graph, node1, node2)\n",
    "                                dist = 0\n",
    "                                for n in range(len(path)-1):\n",
    "                                    edge_data = phylo_graph.get_edge_data(path[n], path[n+1])\n",
    "                                    dist += float(edge_data.get(\"weight\", 1.0))\n",
    "                                GraphDistMatrix[feat1][feat2] = dist\n",
    "                            except nx.NetworkXNoPath:\n",
    "                                GraphDistMatrix[feat1][feat2] = float('inf')\n",
    "                        \n",
    "                        if (i + 1) % 1000 == 0:\n",
    "                            print(f\"  Progress: {i+1}/{len(network_features)}\")\n",
    "                    \n",
    "                    print(f\"GraphDistMatrix: {len(GraphDistMatrix)} x {len(GraphDistMatrix)} features\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error computing GraphDistMatrix: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                GraphDistMatrix = None\n",
    "\n",
    "            print(f\"\\n    Saving distance matrices...\")\n",
    "            matrices_output = Path(config[\"phylogeny_output\"]) / f\"MATRICES_{DISEASE}.pickle\"\n",
    "            \n",
    "            matrices_to_save = {}\n",
    "            if TreeDistMatrix is not None:\n",
    "                matrices_to_save[\"TreeDistMatrix\"] = TreeDistMatrix\n",
    "                print(f\"TreeDistMatrix included\")\n",
    "            if SeqDistMatrix is not None:\n",
    "                matrices_to_save[\"SeqDistMatrix\"] = SeqDistMatrix\n",
    "                print(f\"SeqDistMatrix included\")\n",
    "            if GraphDistMatrix is not None:\n",
    "                matrices_to_save[\"GraphDistMatrix\"] = GraphDistMatrix\n",
    "                print(f\"GraphDistMatrix included\")\n",
    "            \n",
    "            if matrices_to_save:\n",
    "                with open(matrices_output, \"wb\") as f:\n",
    "                    pickle.dump(matrices_to_save, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(f\"\\n    Distance matrices saved to: {matrices_output}\")\n",
    "                print(f\" Total matrices: {len(matrices_to_save)}\")\n",
    "            else:\n",
    "                print(f\" No distance matrices to save\")\n",
    "\n",
    "        try:\n",
    "            checkpoint_path = Path(config[\"output_dir\"]) / \"checkpoint_data.pkl\"\n",
    "            if checkpoint_path.exists():\n",
    "                with open(checkpoint_path, 'rb') as f:\n",
    "                    checkpoint_data = pickle.load(f)\n",
    "                \n",
    "                checkpoint_data.update({\n",
    "                    'phylogeny_tip_count': total_tips,\n",
    "                    'distance_matrix_created': True,\n",
    "                    'num_distance_matrices': len(matrices_to_save) if 'matrices_to_save' in locals() else 0\n",
    "                })\n",
    "                with open(checkpoint_path, 'wb') as f:\n",
    "                    pickle.dump(checkpoint_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(f\" Checkpoint updated\")\n",
    "        except Exception as e:\n",
    "            print(f\" Could not update checkpoint: {e}\")\n",
    "\n",
    "        print(\"\\n    Distance matrix generation complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error in distance matrix generation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\" Phylogeny file not found: {phylogeny_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03301114",
   "metadata": {},
   "source": [
    "## Generate Output Files\n",
    "### Save metadata and BIOM tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                               \n",
    "                     \n",
    "                                                                               \n",
    "print(f\"Saving metadata files for {DISEASE}...\")\n",
    "\n",
    "case_metadata = cases_valid.copy()\n",
    "case_metadata['case_control'] = DISEASE.upper()\n",
    "case_metadata['group'] = 'case'\n",
    "\n",
    "control_metadata = matched_controls.copy()\n",
    "control_metadata['case_control'] = 'Control'\n",
    "control_metadata['group'] = 'control'\n",
    "\n",
    "combined_metadata = pd.concat([case_metadata, control_metadata], ignore_index=True)\n",
    "\n",
    "                       \n",
    "output_cols = [sample_col, age_col, bmi_col, sex_col, disease_col, 'case_control', 'group']\n",
    "if type_col:\n",
    "    output_cols.append(type_col)\n",
    "\n",
    "available_cols = [col for col in output_cols if col in combined_metadata.columns]\n",
    "final_metadata = combined_metadata[available_cols].copy()\n",
    "\n",
    "metadata_path = Path(config[\"metadata_output\"])\n",
    "final_metadata.to_csv(metadata_path / f\"AGP_{DISEASE}_metadata.txt\", sep=\"\\t\", index=False)\n",
    "case_metadata[available_cols].to_csv(metadata_path / f\"AGP_{DISEASE}_cases_metadata.txt\", sep=\"\\t\", index=False)\n",
    "control_metadata[available_cols].to_csv(metadata_path / f\"AGP_{DISEASE}_controls_metadata.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "                                                       \n",
    "with open(metadata_path / f\"samples_{DISEASE.lower()}_cases.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(cases_in_biom)) + \"\\n\")\n",
    "\n",
    "with open(metadata_path / f\"samples_{DISEASE.lower()}_controls.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(control_in_biom)) + \"\\n\")\n",
    "\n",
    "print(f\"✓ Metadata saved to: {metadata_path}\")\n",
    "print(f\"  Combined dataset: {len(final_metadata):,} samples\")\n",
    "print(f\"  {DISEASE} cases: {len(case_metadata):,}\")\n",
    "print(f\"  Controls: {len(control_metadata):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783de25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                               \n",
    "                         \n",
    "                                                                               \n",
    "print(f\"Saving BIOM tables for {DISEASE}...\")\n",
    "\n",
    "biom_path = Path(config[\"biom_output\"])\n",
    "\n",
    "if case_biom is not None:\n",
    "    print(f\"  Processing {DISEASE} BIOM table...\")\n",
    "    \n",
    "    case_tsv_path = biom_path / f\"AGP_{DISEASE}_cases.tsv\"\n",
    "    print(f\"  Converting to TSV...\")\n",
    "    \n",
    "    case_df = case_biom.to_dataframe()\n",
    "    case_df = case_df.T\n",
    "    case_df.index.name = \"#SampleID\"\n",
    "    case_df.to_csv(case_tsv_path, sep=\"\\t\")\n",
    "    \n",
    "    print(f\"✓ {DISEASE} TSV saved: {case_tsv_path}\")\n",
    "\n",
    "if control_biom is not None:\n",
    "    print(f\"  Processing Control BIOM table...\")\n",
    "    \n",
    "    control_tsv_path = biom_path / f\"AGP_{DISEASE}_controls.tsv\"\n",
    "    print(f\"  Converting to TSV...\")\n",
    "    \n",
    "    control_df = control_biom.to_dataframe()\n",
    "    control_df = control_df.T\n",
    "    control_df.index.name = \"#SampleID\"\n",
    "    control_df.to_csv(control_tsv_path, sep=\"\\t\")\n",
    "    \n",
    "    print(f\"✓ Control TSV saved: {control_tsv_path}\")\n",
    "\n",
    "print(f\"\\n✓ BIOM table processing complete!\")\n",
    "print(f\"→ TSV files are ready for graph construction pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Updating checkpoint with new variables...\")\n",
    "\n",
    "checkpoint_path = Path(config[\"output_dir\"]) / \"checkpoint_data.pkl\"\n",
    "if checkpoint_path.exists():\n",
    "    with open(checkpoint_path, 'rb') as f:\n",
    "        checkpoint_data = pickle.load(f)\n",
    "\n",
    "    biom_path = Path(config[\"biom_output\"]) if \"biom_output\" in config else Path(config[\"output_dir\"]) / \"biom_tables\"\n",
    "    disease_tsv_path = biom_path / f\"AGP_{DISEASE}_cases.tsv\"\n",
    "    control_tsv_path = biom_path / f\"AGP_{DISEASE}_controls.tsv\"\n",
    "    all_tsv_files_created = disease_tsv_path.exists() and control_tsv_path.exists()\n",
    "\n",
    "    phylogeny_output_dir = Path(config[\"phylogeny_output\"]) if \"phylogeny_output\" in config else Path(config[\"output_dir\"]) / \"phylogeny\"\n",
    "    phylogeny_copied_path = phylogeny_output_dir / f\"phylogeny_{DISEASE}.nwk\"\n",
    "    matrices_output_path = phylogeny_output_dir / f\"MATRICES_{DISEASE}.pickle\"\n",
    "\n",
    "    sequences_output_dir = Path(config[\"sequences_output\"]) if \"sequences_output\" in config else Path(config[\"output_dir\"]) / \"sequences\"\n",
    "    sequences_output_path = sequences_output_dir / f\"ASV_sequences_{DISEASE}.fasta\"\n",
    "\n",
    "    checkpoint_data.update({\n",
    "        'phylogeny_copied': phylogeny_copied_path.exists(),\n",
    "        'distance_matrix_created': matrices_output_path.exists(),\n",
    "        'sequences_extracted': sequences_output_path.exists(),\n",
    "        'all_tsv_files_created': all_tsv_files_created\n",
    "    })\n",
    "    \n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(checkpoint_data, f)\n",
    "    \n",
    "    print(f\" Checkpoint updated with new variables\")\n",
    "    print(f\" Added variables:\")\n",
    "    print(f\"- phylogeny_copied\")\n",
    "    print(f\"- distance_matrix_created\") \n",
    "    print(f\"- sequences_extracted\")\n",
    "    print(f\"- all_tsv_files_created\")\n",
    "else:\n",
    "    print(f\" No existing checkpoint found to update\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90780754",
   "metadata": {},
   "source": [
    "## Pipeline Summary\n",
    "### Outputs overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{DISEASE} Data Extraction Pipeline Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n Dataset Summary:\")\n",
    "print(f\"Total {DISEASE} cases: {len(cases_valid):,}\")\n",
    "print(f\"Total matched controls: {len(matched_controls):,}\")\n",
    "print(f\"Total samples in analysis: {len(cases_valid) + len(matched_controls):,}\")\n",
    "\n",
    "if case_biom is not None and control_biom is not None:\n",
    "    print(f\"\\n Microbiome Data:\")\n",
    "    print(f\"{DISEASE} features: {case_biom.shape[0]:,}\")\n",
    "    print(f\"Control features: {control_biom.shape[0]:,}\")\n",
    "    print(f\"{DISEASE} samples with data: {case_biom.shape[1]:,}\")\n",
    "    print(f\"Control samples with data: {control_biom.shape[1]:,}\")\n",
    "\n",
    "print(f\"\\n Output Directory Structure:\")\n",
    "print(f\"{config['output_dir']}/\")\n",
    "print(f\" metadata/\")\n",
    "print(f\"AGP_{DISEASE}_metadata.txt\")\n",
    "print(f\"AGP_{DISEASE}_cases_metadata.txt\")\n",
    "print(f\"AGP_{DISEASE}_controls_metadata.txt\")\n",
    "print(f\"samples_{DISEASE.lower()}_cases.txt\")\n",
    "print(f\"samples_{DISEASE.lower()}_controls.txt\")\n",
    "print(f\" biom_tables/\")\n",
    "print(f\"AGP_{DISEASE}_cases.tsv\")\n",
    "print(f\"AGP_{DISEASE}_controls.tsv\")\n",
    "print(f\" phylogeny/\")\n",
    "print(f\"phylogeny_{DISEASE}.nwk\")\n",
    "print(f\"MATRICES_{DISEASE}.pickle\")\n",
    "print(f\" config/\")\n",
    "print(f\" pipeline_config.json\")\n",
    "\n",
    "print(f\"\\n Next Steps:\")\n",
    "print(f\"1. Use the generated files in your graph construction notebook\")\n",
    "print(f\"2. The TSV files can be used for traditional microbiome analysis\")\n",
    "print(f\"3. The distance matrix pickle is ready for graph-based ML\")\n",
    "\n",
    "print(f\"\\n Pipeline completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}