{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "envcheck-01",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Multi-Disease Phylogenetic Graph Construction Pipeline\n",
    "\n",
    "## Summary\n",
    "- Build per-sample graphs from distance matrices and abundance tables.\n",
    "- Save graphs, labels, and metadata under `{DISEASE}_analysis_output/graphs`.\n",
    "- Optional visualizations and checkpoints.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook constructs per-sample phylogenetic graphs from the abundance\n",
    "tables and distance matrices produced by `01_data_extraction.ipynb`. The target\n",
    "disease is loaded from `pipeline_config.json` (or `EXPERIMENT_CONFIG_PATH`) and\n",
    "outputs are written to `{DISEASE}_analysis_output/graphs`.\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "```\n",
    "Input Data (from 01_data_extraction.ipynb)\n",
    "├── Distance matrix (MATRICES_{DISEASE}.pickle)\n",
    "├── Abundance tables (AGP_{DISEASE}_cases.tsv, AGP_{DISEASE}_controls.tsv)\n",
    "├── Sample ID lists (samples_{disease}_cases.txt, samples_{disease}_controls.txt)\n",
    "└── Optional alignment file ({DISEASE}_seqs_CMaligned_with_Rfam.sto)\n",
    "\n",
    "Graph Construction Pipeline\n",
    "1. Configuration and input validation\n",
    "2. Load distance matrix and abundance tables\n",
    "3. Build base phylogenetic graph\n",
    "4. Generate sample-specific graphs\n",
    "5. Validate graphs and save outputs\n",
    "\n",
    "Output Files\n",
    "├── graphs/\n",
    "│   ├── nx_graphs_{DISEASE}.pkl\n",
    "│   ├── labels_{DISEASE}.npy\n",
    "│   ├── graph_metadata_{DISEASE}.csv\n",
    "│   ├── graph_config_{DISEASE}.json\n",
    "│   └── graph_examples/\n",
    "└── visualizations/\n",
    "    └── statistics/\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- Multi-disease support driven by config\n",
    "- Robust validation and checkpointing\n",
    "- Graph quality control and optional visualizations\n",
    "- Organized outputs for downstream modeling\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed execution of `01_data_extraction.ipynb` for the target disease\n",
    "- Generated files in `{DISEASE}_analysis_output/` directory\n",
    "- Required Python packages installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pip-02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "from skbio import TreeNode\n",
    "from Bio import SeqIO\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch {torch.__version__}\")\n",
    "    print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"\\nGraph Construction - {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"Working dir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadeddc8-7bee-4421-98d6-902139a526f1",
   "metadata": {},
   "source": [
    "## Config setup\n",
    "### Load config and paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35776ee-ab0b-44ad-addb-76276635b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                       \n",
    "_default_disease = os.environ.get(\"DISEASE\", \"IBD\").upper()\n",
    "config_file = Path(f\"{_default_disease}_analysis_output/config/pipeline_config.json\")\n",
    "if config_file.exists():\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    print(\"Loaded configuration from previous notebook\")\n",
    "else:\n",
    "    print(\"No configuration file found - using default settings\")\n",
    "    config = {\n",
    "        \"disease\": \"IBD\",\n",
    "        \"output_dir\": f\"{_default_disease}_analysis_output\"\n",
    "    }\n",
    "\n",
    "DISEASE = config.get(\"disease\", \"IBD\")\n",
    "\n",
    "                                                      \n",
    "if 'EXPERIMENT_CONFIG' in globals() and EXPERIMENT_CONFIG:\n",
    "    exp_disease = EXPERIMENT_CONFIG.get('data_extraction', {}).get('disease', None)\n",
    "    if exp_disease and exp_disease != 'N/A':\n",
    "        DISEASE = exp_disease\n",
    "        print(f'Using disease from experiment config: {DISEASE}')\n",
    "output_dir = Path(config.get(\"output_dir\", f\"{DISEASE}_analysis_output\"))\n",
    "\n",
    "PICKLE_FILE = output_dir / \"phylogeny\" / f\"MATRICES_{DISEASE}.pickle\"\n",
    "CASE_BIOM_TSV = output_dir / \"biom_tables\" / f\"AGP_{DISEASE}_cases.tsv\"\n",
    "CONTROL_BIOM_TSV = output_dir / \"biom_tables\" / f\"AGP_{DISEASE}_controls.tsv\"\n",
    "ALN_STOCKHOLM_FILE = f\"{DISEASE}_seqs_CMaligned_with_Rfam.sto\"\n",
    "\n",
    "CASE_IDS_FILE = output_dir / \"metadata\" / f\"samples_{DISEASE.lower()}_cases.txt\"\n",
    "CONTROL_IDS_FILE = output_dir / \"metadata\" / f\"samples_{DISEASE.lower()}_controls.txt\"\n",
    "\n",
    "graphs_output_dir = output_dir / \"graphs\"\n",
    "visualizations_dir = output_dir / \"visualizations\"\n",
    "checkpoints_dir = output_dir / \"checkpoints\"\n",
    "\n",
    "for dir_path in [graphs_output_dir, visualizations_dir, checkpoints_dir]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "(graphs_output_dir / \"graph_examples\").mkdir(exist_ok=True)\n",
    "(visualizations_dir / \"statistics\").mkdir(exist_ok=True)\n",
    "\n",
    "OUT_GRAPHS_PKL = graphs_output_dir / f\"nx_graphs_{DISEASE}.pkl\"\n",
    "OUT_LABELS_NPY = graphs_output_dir / f\"labels_{DISEASE}.npy\"\n",
    "OUT_METADATA_CSV = graphs_output_dir / f\"graph_metadata_{DISEASE}.csv\"\n",
    "OUT_CONFIG_JSON = graphs_output_dir / f\"graph_config_{DISEASE}.json\"\n",
    "CHECKPOINT_FILE = checkpoints_dir / f\"graph_construction_checkpoint_{DISEASE}.pkl\"\n",
    "\n",
    "print(f\" Disease focus: {DISEASE}\")\n",
    "print(f\" Output directory: {output_dir}\")\n",
    "print(f\" Graph output directory: {graphs_output_dir}\")\n",
    "print(f\" Visualizations directory: {visualizations_dir}\")\n",
    "print(f\" Checkpoints directory: {checkpoints_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = None\n",
    "if 'EXPERIMENT_CONFIG_PATH' in os.environ:\n",
    "    config_path = Path(os.environ['EXPERIMENT_CONFIG_PATH'])\n",
    "    print(f\" Using config from environment: {config_path}\")\n",
    "elif Path(\"config.yaml\").exists():\n",
    "    config_path = Path(\"config.yaml\")\n",
    "    print(f\" Found config in current directory\")\n",
    "\n",
    "if config_path and config_path.exists():\n",
    "    print(f\" Loading experiment configuration from: {config_path}\")\n",
    "    with open(config_path, 'r') as f:\n",
    "        EXPERIMENT_CONFIG = yaml.safe_load(f)\n",
    "    print(f\" Loaded configuration for experiment\")\n",
    "    \n",
    "    # FIX: Look for disease in multiple locations\n",
    "    exp_disease = (\n",
    "        EXPERIMENT_CONFIG.get('disease') or  # Root level\n",
    "        EXPERIMENT_CONFIG.get('data_extraction', {}).get('disease') or  # data_extraction.disease\n",
    "        EXPERIMENT_CONFIG.get('data_extraction', {}).get('disease_criteria', {}).get('disease')  # disease_criteria\n",
    "    )\n",
    "    \n",
    "    gc_config = EXPERIMENT_CONFIG.get('graph_construction', {})\n",
    "    knn_k = gc_config.get('knn', {}).get('k', 'default')\n",
    "    \n",
    "    print(f\"Disease from config: {exp_disease}\")\n",
    "    print(f\"Graph k-NN: k={knn_k}\")\n",
    "else:\n",
    "    print(\" No experiment config found - using default parameters from notebook\")\n",
    "    EXPERIMENT_CONFIG = {}\n",
    "    exp_disease = None\n",
    "\n",
    "# FIX: Update DISEASE immediately after loading config\n",
    "if EXPERIMENT_CONFIG and exp_disease:\n",
    "    DISEASE = exp_disease.upper()  # Ensure uppercase\n",
    "    print(f\"DISEASE updated to: {DISEASE}\")\n",
    "    \n",
    "    # Recalculate ALL paths with new DISEASE (NO notebooks/ prefix - we're already in notebooks/)\n",
    "    output_dir = Path(f\"{DISEASE}_analysis_output\")\n",
    "    graphs_output_dir = output_dir / \"graphs\"\n",
    "    visualizations_dir = output_dir / \"visualizations\"\n",
    "    checkpoints_dir = output_dir / \"checkpoints\"\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_path in [graphs_output_dir, visualizations_dir, checkpoints_dir]:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    PICKLE_FILE = output_dir / \"phylogeny\" / f\"MATRICES_{DISEASE}.pickle\"\n",
    "    CASE_BIOM_TSV = output_dir / \"biom_tables\" / f\"AGP_{DISEASE}_cases.tsv\"\n",
    "    CONTROL_BIOM_TSV = output_dir / \"biom_tables\" / f\"AGP_{DISEASE}_controls.tsv\"\n",
    "    CASE_IDS_FILE = output_dir / \"metadata\" / f\"samples_{DISEASE.lower()}_cases.txt\"\n",
    "    CONTROL_IDS_FILE = output_dir / \"metadata\" / f\"samples_{DISEASE.lower()}_controls.txt\"\n",
    "    \n",
    "    OUT_GRAPHS_PKL = graphs_output_dir / f\"nx_graphs_{DISEASE}.pkl\"\n",
    "    OUT_LABELS_NPY = graphs_output_dir / f\"labels_{DISEASE}.npy\"\n",
    "    OUT_METADATA_CSV = graphs_output_dir / f\"graph_metadata_{DISEASE}.csv\"\n",
    "    OUT_CONFIG_JSON = graphs_output_dir / f\"graph_config_{DISEASE}.json\"\n",
    "    CHECKPOINT_FILE = checkpoints_dir / f\"graph_construction_checkpoint_{DISEASE}.pkl\"\n",
    "    \n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"Graphs output: {OUT_GRAPHS_PKL}\")\n",
    "    print(f\"Data files: {PICKLE_FILE}\")\n",
    "    \n",
    "    # Read FORCE_REBUILD from config\n",
    "    force_rebuild_config = EXPERIMENT_CONFIG.get('graph_construction', {}).get('force_rebuild', False)\n",
    "    if force_rebuild_config:\n",
    "        FORCE_REBUILD = True\n",
    "        print(\"FORCE_REBUILD set to True from config\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_GRAPH_CONSTRUCTION = False\n",
    "# Read FORCE_REBUILD from environment or keep default\n",
    "FORCE_REBUILD = os.environ.get(\"FORCE_REBUILD\", \"False\").lower() in [\"true\", \"1\", \"yes\"]\n",
    "\n",
    "if OUT_GRAPHS_PKL.exists() and OUT_LABELS_NPY.exists() and not FORCE_REBUILD:\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    \n",
    "    try:\n",
    "        with open(OUT_GRAPHS_PKL, 'rb') as f:\n",
    "            existing_graphs = pickle.load(f)\n",
    "        existing_labels = np.load(OUT_LABELS_NPY)\n",
    "        \n",
    "        if len(existing_graphs) > 0 and len(existing_labels) > 0:\n",
    "            print(\"=\"*60)\n",
    "            print(\"EXISTING GRAPHS FOUND - SKIPPING REBUILD\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Graphs file: {OUT_GRAPHS_PKL}\")\n",
    "            print(f\"Number of graphs: {len(existing_graphs)}\")\n",
    "            print(f\"Number of labels: {len(existing_labels)}\")\n",
    "            print(f\"File modified: {OUT_GRAPHS_PKL.stat().st_mtime}\")\n",
    "            print()\n",
    "            print(f\"To force rebuild, set FORCE_REBUILD = True\")\n",
    "            print(\"=\"*60)\n",
    "            SKIP_GRAPH_CONSTRUCTION = True\n",
    "            \n",
    "            GRAPHS_PREPARED = existing_graphs\n",
    "            LABELS = existing_labels\n",
    "    except Exception as e:\n",
    "        print(f\" Existing graphs file is corrupted or invalid: {e}\")\n",
    "        print(f\"Will rebuild graphs from scratch...\")\n",
    "        SKIP_GRAPH_CONSTRUCTION = False\n",
    "\n",
    "if not SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Will build graphs from scratch...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dc3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_PARAMS = {\n",
    "    \"min_nodes\": 8,\n",
    "    \"min_edges\": 5,\n",
    "    \"normalize_weights\": True,\n",
    "    \"max_nodes_per_graph\": 3000,\n",
    "    \"connectivity_check\": False,\n",
    "    \"remove_isolates\": True,\n",
    "    \"parallel_processing\": False,\n",
    "    \"batch_size\": 50,\n",
    "    \"save_intermediate\": True,\n",
    "    \"graph_type\": \"knn\",    # supported: knn, tree, mst, threshold, hierarchical\n",
    "    \"knn_k\": 10,\n",
    "    \"knn_symmetric\": True,\n",
    "    \"knn_max_distance_factor\": 3.5,\n",
    "    \"threshold_percentile\": 25,\n",
    "    \"tree_ancestor_levels\": 3,\n",
    "    \"randomize_edges\": False,\n",
    "    \"preserve_degree\": True,\n",
    "    \"weight_transform\": \"identity\"\n",
    "}\n",
    "\n",
    "gc_config = EXPERIMENT_CONFIG.get('graph_construction', {})\n",
    "\n",
    "if 'graph_type' in gc_config:\n",
    "    GRAPH_PARAMS['graph_type'] = gc_config['graph_type']\n",
    "    print(f\"Using graph_type from config: {gc_config['graph_type']}\")\n",
    "\n",
    "knn_config = gc_config.get('knn', {})\n",
    "if 'k' in knn_config:\n",
    "    GRAPH_PARAMS['knn_k'] = knn_config['k']\n",
    "if 'symmetric' in knn_config:\n",
    "    GRAPH_PARAMS['knn_symmetric'] = knn_config['symmetric']\n",
    "if 'max_distance_factor' in knn_config:\n",
    "    GRAPH_PARAMS['knn_max_distance_factor'] = knn_config['max_distance_factor']\n",
    "\n",
    "threshold_config = gc_config.get('threshold', {})\n",
    "if 'percentile' in threshold_config:\n",
    "    GRAPH_PARAMS['threshold_percentile'] = threshold_config['percentile']\n",
    "\n",
    "tree_config = gc_config.get('tree', {})\n",
    "if 'ancestor_levels' in tree_config:\n",
    "    GRAPH_PARAMS['tree_ancestor_levels'] = tree_config['ancestor_levels']\n",
    "\n",
    "quality_config = gc_config.get('quality', {})\n",
    "if 'min_nodes' in quality_config:\n",
    "    GRAPH_PARAMS['min_nodes'] = quality_config['min_nodes']\n",
    "if 'min_edges' in quality_config:\n",
    "    GRAPH_PARAMS['min_edges'] = quality_config['min_edges']\n",
    "\n",
    "edge_config = gc_config.get('edge_construction', {})\n",
    "if 'randomize_edges' in edge_config:\n",
    "    GRAPH_PARAMS['randomize_edges'] = edge_config['randomize_edges']\n",
    "if 'preserve_degree' in edge_config:\n",
    "    GRAPH_PARAMS['preserve_degree'] = edge_config['preserve_degree']\n",
    "\n",
    "weight_config = gc_config.get('weights', {})\n",
    "if 'weight_transform' in weight_config:\n",
    "    GRAPH_PARAMS['weight_transform'] = weight_config['weight_transform']\n",
    "\n",
    "VIZ_PARAMS = {\n",
    "    \"max_nodes_to_plot\": 100,\n",
    "    \"node_size_scale\": 100,\n",
    "    \"edge_width_scale\": 1,\n",
    "    \"figsize\": (12, 8),\n",
    "    \"dpi\": 150,\n",
    "    \"save_plots\": False,\n",
    "    \"show_plots\": False\n",
    "}\n",
    "\n",
    "print(f\" Graph construction parameters:\")\n",
    "for key, value in GRAPH_PARAMS.items():\n",
    "    print(f\"- {key}: {value}\")\n",
    "\n",
    "print(f\"\\n Visualization parameters:\")\n",
    "for key, value in VIZ_PARAMS.items():\n",
    "    print(f\"- {key}: {value}\")\n",
    "\n",
    "print(f\"\\n Expected outputs:\")\n",
    "print(f\"- Graphs: {OUT_GRAPHS_PKL}\")\n",
    "print(f\"- Labels: {OUT_LABELS_NPY}\")\n",
    "print(f\"- Metadata: {OUT_METADATA_CSV}\")\n",
    "print(f\"- Config: {OUT_CONFIG_JSON}\")\n",
    "print(f\"- Checkpoint: {CHECKPOINT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caf6c6",
   "metadata": {},
   "source": [
    "## Checkpoint System and Input Validation\n",
    "\n",
    "### Checkpoint Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_file):\n",
    "    \"\"\"Load checkpoint data if available.\"\"\"\n",
    "    if checkpoint_file.exists():\n",
    "        try:\n",
    "            with open(checkpoint_file, 'rb') as f:\n",
    "                checkpoint_data = pickle.load(f)\n",
    "            print(f\" Loaded checkpoint from: {checkpoint_file}\")\n",
    "            return checkpoint_data\n",
    "        except Exception as e:\n",
    "            print(f\" Error loading checkpoint: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(checkpoint_data, checkpoint_file):\n",
    "    \"\"\"Save checkpoint data.\"\"\"\n",
    "    try:\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(checkpoint_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\" Checkpoint saved to: {checkpoint_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error saving checkpoint: {e}\")\n",
    "\n",
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    checkpoint_data = {'step': 5, 'skipped': True}\n",
    "    print(\"Skipping checkpoint loading - using existing graphs\")\n",
    "else:\n",
    "    checkpoint_data = load_checkpoint(CHECKPOINT_FILE)\n",
    "\n",
    "if checkpoint_data is None:\n",
    "    checkpoint_data = {\n",
    "        'step': 0,\n",
    "        'base_graph': None,\n",
    "        'seq_to_ids': None,\n",
    "        'graphs': [],\n",
    "        'labels': [],\n",
    "        'metadata': [],\n",
    "        'config': config,\n",
    "        'graph_params': GRAPH_PARAMS,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    print(\"Starting fresh - no checkpoint found\")\n",
    "else:\n",
    "    step_names = {\n",
    "        0: \"Initialization\",\n",
    "        1: \"Step 4: Data Loading and Distance Matrix Processing\",\n",
    "        2: \"Step 4: Sequence Mappings (completed)\",\n",
    "        3: \"Step 5: Base Phylogenetic Graph Construction\",\n",
    "        4: \"Step 7: Sample-Specific Graph Construction\",\n",
    "        5: \"Step 9: Output Generation and Pipeline Summary\"\n",
    "    }\n",
    "    \n",
    "    current_step = checkpoint_data.get('step', 0)\n",
    "    current_step_name = step_names.get(current_step, f\"Unknown step {current_step}\")\n",
    "    \n",
    "    print(f\" Checkpoint contains:\")\n",
    "    print(f\"- Checkpoint Step: {current_step}\")\n",
    "    print(f\"- Pipeline Step: {current_step_name}\")\n",
    "    print(f\"- Base graph: {'' if checkpoint_data.get('base_graph') is not None else ''}\")\n",
    "    print(f\"- Sequence mappings: {'' if checkpoint_data.get('seq_to_ids') is not None else ''}\")\n",
    "    print(f\"- Sample graphs: {len(checkpoint_data.get('graphs', []))}\")\n",
    "    print(f\"- Labels: {len(checkpoint_data.get('labels', []))}\")\n",
    "    print(f\"- Timestamp: {checkpoint_data.get('timestamp', 'Unknown')}\")\n",
    "    \n",
    "    next_steps = {\n",
    "        0: \"Step 4: Data Loading and Distance Matrix Processing\",\n",
    "        1: \"Step 4: Sequence Mappings\",\n",
    "        2: \"Step 5: Base Phylogenetic Graph Construction\", \n",
    "        3: \"Step 7: Sample-Specific Graph Construction\",\n",
    "        4: \"Step 9: Output Generation and Pipeline Summary\",\n",
    "        5: \"Pipeline completed! \"\n",
    "    }\n",
    "    \n",
    "    next_step = next_steps.get(current_step, \"Unknown next step\")\n",
    "    print(f\"\\n Next step: {next_step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb3776-5ab9-4d3e-a108-0559f79445cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Skipping - using existing graphs\")\n",
    "else:\n",
    "    if SKIP_GRAPH_CONSTRUCTION:\n",
    "        print(\"Skipping input validation - using existing graphs\")\n",
    "    else:\n",
    "        print(\"Validating input files...\")\n",
    "\n",
    "    required_files = [\n",
    "        (PICKLE_FILE, \"Distance matrix pickle\"),\n",
    "        (CASE_BIOM_TSV, f\"{DISEASE} cases abundance table\"),\n",
    "        (CONTROL_BIOM_TSV, \"Control abundance table\"),\n",
    "        (CASE_IDS_FILE, f\"{DISEASE} sample IDs\"),\n",
    "        (CONTROL_IDS_FILE, \"Control sample IDs\")\n",
    "    ]\n",
    "\n",
    "    optional_files = [\n",
    "        (ALN_STOCKHOLM_FILE, \"Stockholm alignment file\")\n",
    "    ]\n",
    "\n",
    "    missing_files = []\n",
    "    for file_path, description in required_files:\n",
    "        if file_path.exists():\n",
    "            size_mb = file_path.stat().st_size / (1024*1024)\n",
    "            print(f\" {description}: {file_path} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\" {description}: {file_path} - MISSING\")\n",
    "            missing_files.append(file_path)\n",
    "\n",
    "    print(\"\\n Optional files:\")\n",
    "    for file_path, description in optional_files:\n",
    "        path_obj = Path(file_path)\n",
    "        if path_obj.exists():\n",
    "            size_mb = path_obj.stat().st_size / (1024*1024)\n",
    "            print(f\" {description}: {path_obj} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\" {description}: {path_obj} - Not found (optional)\")\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"\\n Missing required files: {len(missing_files)}\")\n",
    "        print(\"Please run the data extraction notebook first to generate these files.\")\n",
    "        raise FileNotFoundError(\"Required input files are missing\")\n",
    "    else:\n",
    "        print(f\"\\n All required files found!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652700a",
   "metadata": {},
   "source": [
    "## Data Loading and Distance Matrix Processing\n",
    "\n",
    "### Load Distance Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb401187",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Skipping distance matrix loading - using existing graphs\")\n",
    "elif checkpoint_data['step'] < 1 or checkpoint_data.get('base_graph') is None:\n",
    "    print(\"Loading phylogenetic distance matrix...\")\n",
    "    try:\n",
    "        with open(PICKLE_FILE, \"rb\") as f:\n",
    "            try:\n",
    "                mats = pickle.load(f)\n",
    "            except ModuleNotFoundError as e:\n",
    "                import sys as _sys\n",
    "                import numpy as _np\n",
    "                if \"numpy._core\" not in _sys.modules:\n",
    "                    _sys.modules[\"numpy._core\"] = _np.core\n",
    "                try:\n",
    "                    import numpy.core.numeric as _numeric\n",
    "                    _sys.modules[\"numpy._core.numeric\"] = _numeric\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    import numpy.core.multiarray as _multiarray\n",
    "                    _sys.modules[\"numpy._core.multiarray\"] = _multiarray\n",
    "                except Exception:\n",
    "                    pass\n",
    "                f.seek(0)\n",
    "                mats = pickle.load(f)\n",
    "        \n",
    "        print(f\"Loaded matrices: {list(mats.keys())}\")\n",
    "        dist_mat = mats.get(\"TreeDistMatrix\")\n",
    "        \n",
    "        if dist_mat is None:\n",
    "            raise ValueError(\"TreeDistMatrix not found in pickle file\")\n",
    "        \n",
    "        print(f\"Distance matrix type: {type(dist_mat)}\")\n",
    "        \n",
    "        if hasattr(dist_mat, 'shape'):\n",
    "            print(f\"Distance matrix shape: {dist_mat.shape}\")\n",
    "        elif isinstance(dist_mat, dict):\n",
    "            print(f\"Distance matrix entries: {len(dist_mat)}\")\n",
    "            if dist_mat:\n",
    "                key0 = next(iter(dist_mat))\n",
    "                print(f\"Example key: {key0} → type: {type(dist_mat[key0])}\")\n",
    "        else:\n",
    "            print(f\"Distance matrix: {dist_mat}\")\n",
    "            \n",
    "        print(f\" Distance matrix loaded successfully\")\n",
    "        \n",
    "        checkpoint_data['dist_mat'] = dist_mat\n",
    "        checkpoint_data['step'] = 1\n",
    "        save_checkpoint(checkpoint_data, CHECKPOINT_FILE)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error loading distance matrix: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"Distance matrix already loaded from checkpoint\")\n",
    "    dist_mat = checkpoint_data['dist_mat']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a876fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Skipping abundance table loading - using existing graphs\")\n",
    "else:\n",
    "    print(\"Loading abundance tables...\")\n",
    "\n",
    "    try:\n",
    "        if CASE_IDS_FILE.exists():\n",
    "            case_ids = pd.read_csv(CASE_IDS_FILE, header=None)[0].astype(str).tolist()\n",
    "            case_ids = [x for x in case_ids if x.strip()]\n",
    "            print(f\"{DISEASE} sample IDs loaded: {len(case_ids):,}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{DISEASE} sample IDs file not found: {CASE_IDS_FILE}\")\n",
    "        \n",
    "        if CONTROL_IDS_FILE.exists():\n",
    "            control_ids = pd.read_csv(CONTROL_IDS_FILE, header=None)[0].astype(str).tolist()\n",
    "            control_ids = [x for x in control_ids if x.strip()]\n",
    "            print(f\"Control sample IDs loaded: {len(control_ids):,}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Control sample IDs file not found: {CONTROL_IDS_FILE}\")\n",
    "        \n",
    "        print(f\"Loading {DISEASE} cases table...\")\n",
    "        df_CASE_raw = pd.read_csv(CASE_BIOM_TSV, sep='\\t', index_col=0)\n",
    "        print(f\"Raw {DISEASE} cases table: {df_CASE_raw.shape[0]:,} rows × {df_CASE_raw.shape[1]:,} columns\")\n",
    "        \n",
    "        print(f\"Loading controls table...\")\n",
    "        df_CONTROL_raw = pd.read_csv(CONTROL_BIOM_TSV, sep='\\t', index_col=0)\n",
    "        print(f\"Raw controls table: {df_CONTROL_raw.shape[0]:,} rows × {df_CONTROL_raw.shape[1]:,} columns\")\n",
    "        \n",
    "        print(\"\\n    Checking table orientation...\")\n",
    "        \n",
    "        case_rows_are_samples = any(str(idx) in case_ids for idx in list(df_CASE_raw.index)[:10])\n",
    "        control_rows_are_samples = any(str(idx) in control_ids for idx in list(df_CONTROL_raw.index)[:10])\n",
    "        \n",
    "        if case_rows_are_samples or control_rows_are_samples:\n",
    "            print(f\"  DETECTED: Tables are TRANSPOSED (rows=samples, columns=features)\")\n",
    "            print(f\" Transposing tables to correct format (rows=features, columns=samples)...\")\n",
    "            df_CASE_raw = df_CASE_raw.T\n",
    "            df_CONTROL_raw = df_CONTROL_raw.T\n",
    "            print(f\"After transpose - {DISEASE} cases: {df_CASE_raw.shape[0]:,} features × {df_CASE_raw.shape[1]:,} samples\")\n",
    "            print(f\"After transpose - Controls: {df_CONTROL_raw.shape[0]:,} features × {df_CONTROL_raw.shape[1]:,} samples\")\n",
    "        else:\n",
    "            print(f\" Tables are in correct orientation (rows=features, columns=samples)\")\n",
    "        \n",
    "        print(f\"\\n    Filtering to {DISEASE}/control samples...\")\n",
    "        \n",
    "        available_case_samples = [col for col in df_CASE_raw.columns if str(col) in case_ids]\n",
    "        df_CASE = df_CASE_raw[available_case_samples]\n",
    "        print(f\"{DISEASE} cases filtered: {df_CASE.shape[0]:,} features × {df_CASE.shape[1]:,} samples\")\n",
    "        print(f\"Found {len(available_case_samples)}/{len(case_ids)} {DISEASE} samples in table\")\n",
    "        \n",
    "        available_control_samples = [col for col in df_CONTROL_raw.columns if str(col) in control_ids]\n",
    "        df_CONTROL = df_CONTROL_raw[available_control_samples]\n",
    "        print(f\"Controls filtered: {df_CONTROL.shape[0]:,} features × {df_CONTROL.shape[1]:,} samples\")\n",
    "        print(f\"Found {len(available_control_samples)}/{len(control_ids)} control samples in table\")\n",
    "        \n",
    "        common_features = set(df_CASE.index) & set(df_CONTROL.index)\n",
    "        print(f\"\\n   Common features between cases and controls: {len(common_features):,}\")\n",
    "        \n",
    "        print(\"\\n    Cleaning feature IDs (removing trailing whitespace)...\")\n",
    "        df_CASE.index = df_CASE.index.str.strip()\n",
    "        df_CONTROL.index = df_CONTROL.index.str.strip()\n",
    "        print(f\" Feature IDs cleaned\")\n",
    "        \n",
    "        print(\"\\n    Final abundance table dimensions:\")\n",
    "        print(f\"   {DISEASE} cases: {df_CASE.shape[0]:,} features × {df_CASE.shape[1]:,} samples\")\n",
    "        print(f\"   Controls: {df_CONTROL.shape[0]:,} features × {df_CONTROL.shape[1]:,} samples\")\n",
    "        print(f\"   Total samples to process: {df_CASE.shape[1] + df_CONTROL.shape[1]:,}\")\n",
    "        \n",
    "        if df_CASE.shape[1] == 0 or df_CONTROL.shape[1] == 0:\n",
    "            raise ValueError(\"No samples found after filtering! Check sample ID matching.\")\n",
    "        \n",
    "        print(\"\\n    Abundance tables loaded and filtered successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error loading abundance tables: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef1e47",
   "metadata": {},
   "source": [
    "### Load Sequence Alignments (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Skipping - using existing graphs\")\n",
    "else:\n",
    "    if checkpoint_data['step'] < 2 or checkpoint_data.get('seq_to_ids') is None:\n",
    "        seq_to_ids = {}\n",
    "        stockholm_available = False\n",
    "\n",
    "        if Path(ALN_STOCKHOLM_FILE).exists():\n",
    "            print(\"Loading Stockholm alignment file...\")\n",
    "            try:\n",
    "                aln_dict = SeqIO.to_dict(SeqIO.parse(ALN_STOCKHOLM_FILE, \"stockholm\"))\n",
    "                seq_to_ids = {str(v.seq).replace(\"-\", \"\").replace(\"U\", \"T\"): k for k, v in aln_dict.items()}\n",
    "                print(f\"Aligned sequences: {len(seq_to_ids):,}\")\n",
    "                stockholm_available = True\n",
    "                print(f\" Stockholm alignment loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\" Error loading Stockholm alignment: {e}\")\n",
    "                print(f\" Continuing without sequence alignment mapping\")\n",
    "        else:\n",
    "            print(\"Stockholm alignment file not found - using direct feature mapping\")\n",
    "            print(f\"This may reduce graph quality if sequences don't match phylogenetic IDs\")\n",
    "\n",
    "        if not stockholm_available:\n",
    "            print(\"Creating fallback sequence mapping...\")\n",
    "            all_features = set(df_CASE.index) | set(df_CONTROL.index)\n",
    "            seq_to_ids = {feature_id: feature_id for feature_id in all_features}\n",
    "            print(f\"Fallback mappings: {len(seq_to_ids):,}\")\n",
    "\n",
    "        print(f\" Total sequence mappings: {len(seq_to_ids):,}\")\n",
    "\n",
    "        print(\"\\n Feature ID diagnostic check:\")\n",
    "        sample_features = list(seq_to_ids.keys())[:5]\n",
    "        for i, feat_id in enumerate(sample_features):\n",
    "            print(f\"[{i}] Length={len(feat_id):3d}: {feat_id[:80]}...\" if len(feat_id) > 80 else f\"   [{i}] Length={len(feat_id):3d}: {feat_id}\")\n",
    "\n",
    "        checkpoint_data['seq_to_ids'] = seq_to_ids\n",
    "        checkpoint_data['stockholm_available'] = stockholm_available\n",
    "        checkpoint_data['step'] = 2\n",
    "        save_checkpoint(checkpoint_data, CHECKPOINT_FILE)\n",
    "    else:\n",
    "        print(\"Sequence mappings already loaded from checkpoint\")\n",
    "        seq_to_ids = checkpoint_data['seq_to_ids']\n",
    "        stockholm_available = checkpoint_data.get('stockholm_available', False)\n",
    "        print(f\"Total sequence mappings: {len(seq_to_ids):,}\")\n",
    "        print(f\"Stockholm alignment available: {stockholm_available}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6209880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_graph_pruning(G, max_edges=None, distance_threshold=None):\n",
    "    \"\"\"\n",
    "    Efficiently prune a graph using multiple strategies:\n",
    "    1. Distance threshold filtering (if provided)\n",
    "    2. Minimum spanning tree construction\n",
    "    3. Optional edge limit\n",
    "    \n",
    "    Parameters:\n",
    "    - G: NetworkX graph\n",
    "    - max_edges: Maximum number of edges to keep (optional)\n",
    "    - distance_threshold: Remove edges with weight > threshold (optional)\n",
    "    \n",
    "    Returns:\n",
    "    - Pruned NetworkX graph\n",
    "    \"\"\"\n",
    "    print(f\"Starting with {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
    "    \n",
    "    if distance_threshold is not None:\n",
    "        print(f\"Applying distance threshold: {distance_threshold:.4f}\")\n",
    "        edges_to_remove = [(u, v) for u, v, data in G.edges(data=True) \n",
    "                          if data.get('weight', 0) > distance_threshold]\n",
    "        \n",
    "        G.remove_edges_from(edges_to_remove)\n",
    "        print(f\"After threshold filtering: {G.number_of_edges():,} edges (removed {len(edges_to_remove):,})\")\n",
    "        \n",
    "        isolated = list(nx.isolates(G))\n",
    "        if isolated:\n",
    "            G.remove_nodes_from(isolated)\n",
    "            print(f\"Removed {len(isolated):,} isolated nodes\")\n",
    "    \n",
    "    if max_edges is None or G.number_of_edges() <= max_edges:\n",
    "        print(f\"Graph already within target size: {G.number_of_edges():,} edges\")\n",
    "        return G\n",
    "    \n",
    "    print(f\"Computing minimum spanning tree (target: {max_edges:,} edges)...\")\n",
    "    try:\n",
    "        mst = nx.minimum_spanning_tree(G, weight='weight', algorithm='kruskal')\n",
    "        print(f\"MST has {mst.number_of_edges():,} edges\")\n",
    "        \n",
    "        if mst.number_of_edges() < max_edges:\n",
    "            print(f\"Adding back {max_edges - mst.number_of_edges():,} shortest edges...\")\n",
    "            \n",
    "            mst_edges = set(mst.edges())\n",
    "            remaining_edges = [(u, v, data['weight']) for u, v, data in G.edges(data=True) \n",
    "                             if (u, v) not in mst_edges and (v, u) not in mst_edges]\n",
    "            remaining_edges.sort(key=lambda x: x[2])\n",
    "            \n",
    "            edges_to_add = min(len(remaining_edges), max_edges - mst.number_of_edges())\n",
    "            for u, v, weight in remaining_edges[:edges_to_add]:\n",
    "                mst.add_edge(u, v, weight=weight)\n",
    "            \n",
    "            print(f\"Final graph: {mst.number_of_edges():,} edges\")\n",
    "        \n",
    "        return mst\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error in MST computation: {e}\")\n",
    "        if max_edges is not None and G.number_of_edges() > max_edges:\n",
    "            print(f\"Fallback: limiting to {max_edges:,} shortest edges\")\n",
    "            edges_with_weights = [(u, v, data['weight']) for u, v, data in G.edges(data=True)]\n",
    "            edges_with_weights.sort(key=lambda x: x[2])\n",
    "            \n",
    "            G_limited = nx.Graph()\n",
    "            G_limited.add_nodes_from(G.nodes())\n",
    "            \n",
    "            for u, v, weight in edges_with_weights[:max_edges]:\n",
    "                G_limited.add_edge(u, v, weight=weight)\n",
    "            \n",
    "            print(f\"Fallback graph: {G_limited.number_of_edges():,} edges\")\n",
    "            return G_limited\n",
    "        \n",
    "        return G\n",
    "\n",
    "print(\"Optimized graph pruning function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a76e2e",
   "metadata": {},
   "source": [
    "## Base Phylogenetic Graph Construction\n",
    "\n",
    "### Build Base Graph from Distance Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Skipping - using existing graphs\")\n",
    "else:\n",
    "    if checkpoint_data['step'] < 3 or checkpoint_data.get('base_graph') is None:\n",
    "        graph_type = GRAPH_PARAMS.get('graph_type', 'knn')\n",
    "        print(f\"Building base phylogenetic graph (type: {graph_type})...\")\n",
    "\n",
    "        try:\n",
    "            # For non-kNN graph types, use src/graph_utils functions\n",
    "            if graph_type in ['threshold', 'mst', 'tree']:\n",
    "                print(f\"Using {graph_type} graph builder from src/graph_utils\")\n",
    "                \n",
    "                # Convert distance matrix to numpy format if needed\n",
    "                if hasattr(dist_mat, 'data') and hasattr(dist_mat, 'ids'):\n",
    "                    dm_numpy = dist_mat.data\n",
    "                    node_ids = list(dist_mat.ids)\n",
    "                elif isinstance(dist_mat, dict):\n",
    "                    # Handle nested dict format - extract TreeDistMatrix\n",
    "                    first_key = next(iter(dist_mat.keys()))\n",
    "                    if 'DistMatrix' in str(first_key):\n",
    "                        nested_dist = dist_mat.get('TreeDistMatrix', list(dist_mat.values())[0])\n",
    "                    else:\n",
    "                        nested_dist = dist_mat\n",
    "                    \n",
    "                    node_ids = list(nested_dist.keys())\n",
    "                    n = len(node_ids)\n",
    "                    dm_numpy = np.zeros((n, n))\n",
    "                    node_to_idx = {nid: i for i, nid in enumerate(node_ids)}\n",
    "                    \n",
    "                    for node1, neighbors in tqdm(nested_dist.items(), desc=\"Converting to numpy\"):\n",
    "                        i = node_to_idx[node1]\n",
    "                        for node2, dist in neighbors.items():\n",
    "                            if node2 in node_to_idx:\n",
    "                                j = node_to_idx[node2]\n",
    "                                dm_numpy[i, j] = dist\n",
    "                                dm_numpy[j, i] = dist\n",
    "                    print(f\"Converted to numpy: {dm_numpy.shape}\")\n",
    "                else:\n",
    "                    dm_numpy = dist_mat\n",
    "                    node_ids = [f\"node_{i}\" for i in range(dm_numpy.shape[0])]\n",
    "                \n",
    "                # Build graph based on type\n",
    "                if graph_type == 'threshold':\n",
    "                    from src.graph_utils import build_threshold_graph\n",
    "                    percentile = GRAPH_PARAMS.get('threshold_percentile', 25)\n",
    "                    G = build_threshold_graph(dm_numpy, node_ids, threshold_percentile=percentile)\n",
    "                    print(f\"Threshold graph: percentile={percentile}, {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "                \n",
    "                elif graph_type == 'mst':\n",
    "                    from src.graph_utils import build_mst_graph\n",
    "                    G = build_mst_graph(dm_numpy, node_ids)\n",
    "                    print(f\"MST graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "                \n",
    "                elif graph_type == 'tree':\n",
    "                    # Tree requires phylogenetic tree file\n",
    "                    tree_path = output_dir / \"phylogeny\" / f\"phylogeny_{DISEASE}.nwk\"\n",
    "                    if tree_path.exists():\n",
    "                        from src.graph_utils import build_tree_graph\n",
    "                        ancestor_levels = GRAPH_PARAMS.get('tree_ancestor_levels', 3)\n",
    "                        G = build_tree_graph(tree_path, node_ids, ancestor_levels=ancestor_levels)\n",
    "                        print(f\"Tree graph: ancestor_levels={ancestor_levels}, {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "                    else:\n",
    "                        print(f\"Tree file not found: {tree_path}, falling back to k-NN\")\n",
    "                        graph_type = 'knn'  # Fall back to kNN\n",
    "                \n",
    "                # Save checkpoint if successful\n",
    "                if graph_type != 'knn':\n",
    "                    checkpoint_data['base_graph'] = G\n",
    "                    checkpoint_data['step'] = 3\n",
    "                    save_checkpoint(checkpoint_data, CHECKPOINT_FILE)\n",
    "                    print(f\"Base {graph_type} graph constructed successfully\")\n",
    "            \n",
    "            # Default k-NN implementation (original code)\n",
    "            if graph_type == 'knn':\n",
    "                G = nx.Graph()\n",
    "            \n",
    "                nested_dist = None\n",
    "\n",
    "                if isinstance(dist_mat, dict):\n",
    "                    print(f\"Processing dictionary-based distance matrix...\")\n",
    "\n",
    "                    first_key = next(iter(dist_mat.keys()))\n",
    "                if isinstance(first_key, str) and 'DistMatrix' in first_key:\n",
    "                    primary_matrix = EXPERIMENT_CONFIG.get('data_extraction', {}).get('distance_matrices', {}).get('primary_matrix', 'TreeDistMatrix')\n",
    "                    print(f\"Selecting primary matrix: {primary_matrix}\")\n",
    "                    if primary_matrix not in dist_mat:\n",
    "                        available = list(dist_mat.keys())\n",
    "                        print(f\"WARNING: {primary_matrix} not found, using {available[0]}\")\n",
    "                        primary_matrix = available[0]\n",
    "                    nested_dist = dist_mat[primary_matrix]\n",
    "                else:\n",
    "                    nested_dist = dist_mat\n",
    "\n",
    "                edge_count = 0\n",
    "                first_val = next(iter(nested_dist.values()))\n",
    "                if isinstance(first_val, dict):\n",
    "                    print(f\"Structure: nested dict (node -> {{neighbor -> distance}})\")\n",
    "                    for node1, neighbors in tqdm(nested_dist.items(), desc=\"Adding edges\"):\n",
    "                        for node2, distance in neighbors.items():\n",
    "                            if node1 != node2:\n",
    "                                G.add_edge(node1, node2, weight=distance)\n",
    "                                edge_count += 1\n",
    "                else:\n",
    "                    print(f\"Structure: flat dict ((node1, node2) -> distance)\")\n",
    "                    for (node1, node2), distance in tqdm(nested_dist.items(), desc=\"Adding edges\"):\n",
    "                        if node1 != node2:\n",
    "                            G.add_edge(node1, node2, weight=distance)\n",
    "                            edge_count += 1\n",
    "                print(f\"Added {edge_count:,} edges from distance matrix\")\n",
    "            else:\n",
    "                # Handle both matrix and dict formats for distance matrix\n",
    "                G = nx.Graph()  # Create graph for phylogenetic type\n",
    "                \n",
    "                if isinstance(dist_mat, dict):\n",
    "                    print(f\"Processing dictionary-based distance matrix (else branch)...\")\n",
    "                    # dist_mat is a dict, use kNN logic instead of matrix logic\n",
    "                    nested_dist = dist_mat\n",
    "                    \n",
    "                    # Get node IDs from dict keys\n",
    "                    node_ids = list(nested_dist.keys())\n",
    "                    print(f\"  Found {len(node_ids):,} nodes in distance dict\")\n",
    "                    \n",
    "                    # Build graph from dict (same as kNN branch)\n",
    "                    edge_count = 0\n",
    "                    first_val = next(iter(nested_dist.values()))\n",
    "                    if isinstance(first_val, dict):\n",
    "                        for node1, neighbors in tqdm(nested_dist.items(), desc=\"Adding edges\"):\n",
    "                            for node2, distance in neighbors.items():\n",
    "                                if node1 != node2:\n",
    "                                    G.add_edge(node1, node2, weight=distance)\n",
    "                                    edge_count += 1\n",
    "                    else:\n",
    "                        for (node1, node2), distance in tqdm(nested_dist.items(), desc=\"Adding edges\"):\n",
    "                            if node1 != node2:\n",
    "                                G.add_edge(node1, node2, weight=distance)\n",
    "                                edge_count += 1\n",
    "                    print(f\"  Added {edge_count:,} edges from distance dict\")\n",
    "                else:\n",
    "                    print(f\"Processing matrix-based distance matrix...\")\n",
    "\n",
    "                    if hasattr(dist_mat, 'ids'):\n",
    "                        node_ids = list(dist_mat.ids)\n",
    "                        print(f\" Extracted {len(node_ids):,} feature IDs from distance matrix\")\n",
    "                        print(f\"Sample IDs: {node_ids[0][:80]}...\" if len(node_ids[0]) > 80 else f\"   Sample IDs: {node_ids[0]}\")\n",
    "                    else:\n",
    "                        print(f\"  WARNING: Distance matrix has no .ids attribute - using generic node names\")\n",
    "                        node_ids = [f\"node_{i}\" for i in range(dist_mat.shape[0])]\n",
    "\n",
    "                    if hasattr(dist_mat, 'todense'):\n",
    "                        dist_dense = dist_mat.todense()\n",
    "                    elif hasattr(dist_mat, 'data'):\n",
    "                        dist_dense = dist_mat.data\n",
    "                    else:\n",
    "                        dist_dense = dist_mat\n",
    "\n",
    "                    n_nodes = dist_dense.shape[0]\n",
    "                    for i in tqdm(range(n_nodes), desc=\"Processing matrix rows\"):\n",
    "                        for j in range(i+1, n_nodes):\n",
    "                            if dist_dense[i, j] > 0:\n",
    "                                G.add_edge(node_ids[i], node_ids[j], weight=float(dist_dense[i, j]))\n",
    "\n",
    "            print(f\"Initial graph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
    "            print(f\"Connected: {nx.is_connected(G)}\")\n",
    "\n",
    "            print(f\"Building sparse k-NN graph (t2d-style)...\")\n",
    "            k = GRAPH_PARAMS.get(\"knn_k\", 8)\n",
    "            symmetric = GRAPH_PARAMS.get(\"knn_symmetric\", True)\n",
    "            max_factor = GRAPH_PARAMS.get(\"knn_max_distance_factor\", None)\n",
    "\n",
    "            if hasattr(dist_mat, 'ids') and hasattr(dist_mat, 'data'):\n",
    "                node_ids = list(dist_mat.ids)\n",
    "                dm = dist_mat.data\n",
    "                use_numpy_dm = True\n",
    "            elif nested_dist is not None and isinstance(nested_dist, dict):\n",
    "                node_ids = list(nested_dist.keys())\n",
    "                print(f\"Using nested dict format with {len(node_ids):,} nodes\")\n",
    "                use_numpy_dm = False\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported distance matrix type for k-NN build\")\n",
    "\n",
    "            import numpy as _np\n",
    "            n = len(node_ids)\n",
    "            knn_edges = []\n",
    "\n",
    "            if use_numpy_dm:\n",
    "                global_median = _np.median(dm[_np.triu_indices(n, k=1)])\n",
    "                max_allowed = None if max_factor is None else global_median * float(max_factor)\n",
    "\n",
    "                for i in range(n):\n",
    "                    row = dm[i].astype(float)\n",
    "                    row[i] = _np.inf\n",
    "                    if k < n - 1:\n",
    "                        idx = _np.argpartition(row, k)[:k]\n",
    "                    else:\n",
    "                        idx = _np.argsort(row)[:k]\n",
    "                    for j in idx:\n",
    "                        w = float(dm[i, j])\n",
    "                        if max_allowed is not None and w > max_allowed:\n",
    "                            continue\n",
    "                        knn_edges.append((node_ids[i], node_ids[j], w))\n",
    "            else:\n",
    "                print(f\"Building k-NN from nested dict (k={k})...\")\n",
    "                all_distances = []\n",
    "                for node1, neighbors in nested_dist.items():\n",
    "                    for node2, dist in neighbors.items():\n",
    "                        if node1 < node2:\n",
    "                            all_distances.append(dist)\n",
    "                if all_distances:\n",
    "                    global_median = _np.median(all_distances)\n",
    "                    max_allowed = None if max_factor is None else global_median * float(max_factor)\n",
    "                    print(f\"Median distance: {global_median:.4f}, max_allowed: {max_allowed}\")\n",
    "                else:\n",
    "                    max_allowed = None\n",
    "\n",
    "                for node1 in tqdm(node_ids, desc=\"Building k-NN\"):\n",
    "                    if node1 not in nested_dist:\n",
    "                        continue\n",
    "                    neighbors = nested_dist[node1]\n",
    "                    sorted_neighbors = sorted(neighbors.items(), key=lambda x: x[1])[:k]\n",
    "                    for node2, w in sorted_neighbors:\n",
    "                        if max_allowed is not None and w > max_allowed:\n",
    "                            continue\n",
    "                        knn_edges.append((node1, node2, w))\n",
    "\n",
    "            if symmetric:\n",
    "                edge_set = {}\n",
    "                for u, v, w in knn_edges:\n",
    "                    a, b = (u, v) if u < v else (v, u)\n",
    "                    if (a, b) not in edge_set or w < edge_set[(a, b)]:\n",
    "                        edge_set[(a, b)] = w\n",
    "                G = nx.Graph()\n",
    "                G.add_nodes_from(node_ids)\n",
    "                for (u, v), w in edge_set.items():\n",
    "                    G.add_edge(u, v, weight=w)\n",
    "            else:\n",
    "                G = nx.Graph()\n",
    "                G.add_nodes_from(node_ids)\n",
    "                for u, v, w in knn_edges:\n",
    "                    G.add_edge(u, v, weight=w)\n",
    "\n",
    "            print(f\"k-NN graph: k={k}, symmetric={symmetric}, nodes={G.number_of_nodes():,}, edges={G.number_of_edges():,}\")\n",
    "            print(f\"Connected: {nx.is_connected(G)}\")\n",
    "\n",
    "            print(f\" Base phylogenetic graph constructed successfully\")\n",
    "\n",
    "            if G.number_of_nodes() > 0:\n",
    "                avg_degree = sum(dict(G.degree()).values()) / G.number_of_nodes()\n",
    "                print(f\"Average degree: {avg_degree:.2f}\")\n",
    "\n",
    "                isolated = list(nx.isolates(G))\n",
    "                if isolated:\n",
    "                    print(f\" Isolated nodes: {len(isolated)}\")\n",
    "                    G.remove_nodes_from(isolated)\n",
    "                    print(f\"After removing isolated nodes: {G.number_of_nodes():,} nodes\")\n",
    "\n",
    "            print(f\" Base phylogenetic graph constructed successfully\")\n",
    "\n",
    "            print(f\"\\n    Performance Estimates:\")\n",
    "            total_samples = len(df_CASE.columns) + len(df_CONTROL.columns)\n",
    "            est_time_per_sample = G.number_of_edges() * 0.00001\n",
    "            est_total_minutes = (est_time_per_sample * total_samples) / 60\n",
    "\n",
    "            print(f\"   - Total samples to process: {total_samples:,}\")\n",
    "            print(f\"   - Est. time per sample: ~{est_time_per_sample:.2f} seconds\")\n",
    "            print(f\"   - Est. total processing time: ~{est_total_minutes:.1f} minutes\")\n",
    "\n",
    "            if G.number_of_edges() > 200_000:\n",
    "                print(f\"\\n        WARNING: Base graph may be large for per-sample processing!\")\n",
    "                print(f\"   Edges: {G.number_of_edges():,}\")\n",
    "                print(f\"   Consider reducing GRAPH_PARAMS['knn_k'] or lowering 'knn_max_distance_factor'\")\n",
    "\n",
    "            checkpoint_data['base_graph'] = G\n",
    "            checkpoint_data['step'] = 3\n",
    "            save_checkpoint(checkpoint_data, CHECKPOINT_FILE)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error building base graph: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(\"Base phylogenetic graph already built from checkpoint\")\n",
    "        G = checkpoint_data['base_graph']\n",
    "        print(f\"Graph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
    "        print(f\"Connected: {nx.is_connected(G)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b045916",
   "metadata": {},
   "source": [
    "## Graph Adjustment Function\n",
    "\n",
    "### Define Graph Adjustment Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_EDGE_WEIGHT_WARNING_SHOWN = False\n",
    "\n",
    "def adjust_graph_to_abundance(G_in, abundance_dict, params=GRAPH_PARAMS):\n",
    "    \"\"\"\n",
    "    Adjust the base phylogenetic graph based on sample-specific abundance data.\n",
    "    OPTIMIZED: Removed expensive edge sorting for significant speedup.\n",
    "    \n",
    "    Now supports edge weight strategies including abundance-based ones:\n",
    "    - identity, inverse, exponential, binary (simple transforms)\n",
    "    - abundance_product, abundance_geometric, abundance_log, etc. (abundance-aware)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G_in : networkx.Graph\n",
    "        Base phylogenetic graph (edge weights = phylogenetic distances)\n",
    "    abundance_dict : dict\n",
    "        Dictionary mapping phylogenetic IDs to abundance values\n",
    "    params : dict\n",
    "        Graph construction parameters including 'weight_transform' strategy\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    networkx.Graph\n",
    "        Adjusted graph for the specific sample with appropriate edge weights\n",
    "    \"\"\"\n",
    "    global _EDGE_WEIGHT_WARNING_SHOWN\n",
    "    \n",
    "    G = G_in.copy()\n",
    "    \n",
    "    nodes_to_remove = [node for node in G.nodes() if abundance_dict.get(node, 0) == 0]\n",
    "    \n",
    "    G.remove_nodes_from(nodes_to_remove)\n",
    "    \n",
    "    if G.number_of_nodes() == 0:\n",
    "        return G\n",
    "    \n",
    "                                  \n",
    "    for node in G.nodes():\n",
    "        G.nodes[node][\"weight\"] = [abundance_dict.get(node, 0)]\n",
    "    \n",
    "    if params.get(\"remove_isolates\", True):\n",
    "        isolated = list(nx.isolates(G))\n",
    "        if isolated:\n",
    "            G.remove_nodes_from(isolated)\n",
    "    \n",
    "                                                                               \n",
    "                                      \n",
    "                                                    \n",
    "                                                                               \n",
    "    weight_strategy = params.get(\"weight_transform\", \"identity\")\n",
    "    \n",
    "    if G.number_of_edges() > 0:\n",
    "                                                   \n",
    "        if weight_strategy.startswith(\"abundance_\"):\n",
    "                                          \n",
    "            edge_weights_applied = False\n",
    "            try:\n",
    "                from src.edge_weights import get_edge_weight_function\n",
    "                weight_fn = get_edge_weight_function(weight_strategy)\n",
    "                \n",
    "                                                                     \n",
    "                for u, v, data in G.edges(data=True):\n",
    "                    dist = data.get(\"weight\", 1.0)                              \n",
    "                    a1 = abundance_dict.get(u, 0)\n",
    "                    a2 = abundance_dict.get(v, 0)\n",
    "                    data[\"weight\"] = weight_fn(dist, a1, a2)\n",
    "                edge_weights_applied = True\n",
    "            except ImportError:\n",
    "                if not _EDGE_WEIGHT_WARNING_SHOWN:\n",
    "                    print(f\"⚠ Could not import edge_weights module, falling back to identity\")\n",
    "                    _EDGE_WEIGHT_WARNING_SHOWN = True\n",
    "            except Exception as e:\n",
    "                if not _EDGE_WEIGHT_WARNING_SHOWN:\n",
    "                    print(f\"⚠ Error applying {weight_strategy}: {e}, falling back to identity\")\n",
    "                    _EDGE_WEIGHT_WARNING_SHOWN = True\n",
    "            \n",
    "                                                                                      \n",
    "            if not edge_weights_applied and not _EDGE_WEIGHT_WARNING_SHOWN:\n",
    "                print(f\"  Using identity weighting (original distances as weights)\")\n",
    "                _EDGE_WEIGHT_WARNING_SHOWN = True\n",
    "        \n",
    "        elif weight_strategy == \"inverse\":\n",
    "            for u, v, data in G.edges(data=True):\n",
    "                w = data.get(\"weight\", 1.0)\n",
    "                data[\"weight\"] = 1.0 / (w + 1e-8)\n",
    "        \n",
    "        elif weight_strategy == \"exponential\":\n",
    "            import math\n",
    "            for u, v, data in G.edges(data=True):\n",
    "                w = data.get(\"weight\", 1.0)\n",
    "                data[\"weight\"] = math.exp(-w)\n",
    "        \n",
    "        elif weight_strategy == \"binary\":\n",
    "            for u, v, data in G.edges(data=True):\n",
    "                data[\"weight\"] = 1.0\n",
    "        \n",
    "                                                                  \n",
    "    \n",
    "                            \n",
    "    if params.get(\"normalize_weights\", True) and G.number_of_edges() > 0:\n",
    "        edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]\n",
    "        max_weight = max(edge_weights) if edge_weights else 1.0\n",
    "        if max_weight > 0:\n",
    "            for u, v, data in G.edges(data=True):\n",
    "                data[\"weight\"] = data[\"weight\"] / max_weight\n",
    "    \n",
    "                            \n",
    "    node_weights = [data[\"weight\"][0] for _, data in G.nodes(data=True) if \"weight\" in data]\n",
    "    if node_weights:\n",
    "        max_node_weight = max(node_weights)\n",
    "        if max_node_weight > 0:\n",
    "            for node, data in G.nodes(data=True):\n",
    "                if \"weight\" in data:\n",
    "                    data[\"weight\"][0] = data[\"weight\"][0] / max_node_weight\n",
    "    \n",
    "    return G\n",
    "\n",
    "print(\"Optimized graph adjustment function defined\")\n",
    "print(f\"Key optimizations:\")\n",
    "print(f\"- Removed expensive edge sorting (5-10x faster)\")\n",
    "print(f\"- Removed redundant per-sample edge pruning\")\n",
    "print(f\"- Optimized node removal with list comprehensions\")\n",
    "print(f\"Parameters: {GRAPH_PARAMS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe8ffd",
   "metadata": {},
   "source": [
    "### Pre-flight Diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Skipping - using existing graphs\")\n",
    "else:\n",
    "    import gc\n",
    "\n",
    "    print(\"Pre-flight Diagnostics for Step 7\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\n Base Graph (k-NN):\")\n",
    "    print(f\"Nodes: {G.number_of_nodes():,}\")\n",
    "    print(f\"Edges: {G.number_of_edges():,}\")\n",
    "    print(f\"Avg degree: {2 * G.number_of_edges() / max(1, G.number_of_nodes()):.2f}\")\n",
    "    print(f\"Graph density: {nx.density(G):.6f}\")\n",
    "\n",
    "    total_samples = len(df_CASE.columns) + len(df_CONTROL.columns)\n",
    "    print(f\"\\n Samples to Process:\")\n",
    "    print(f\"{DISEASE} cases: {len(df_CASE.columns):,}\")\n",
    "    print(f\"Controls: {len(df_CONTROL.columns):,}\")\n",
    "    print(f\"Total: {total_samples:,}\")\n",
    "\n",
    "    if total_samples > 10000:\n",
    "        print(f\"\\n    CRITICAL WARNING: {total_samples:,} samples detected!\")\n",
    "        print(f\"This is unusually high. Expected ~400-1000 samples for disease study.\")\n",
    "        print(f\"  Did the transpose/filter fix in Cell 10 work correctly?\")\n",
    "        print(f\"Please check Cell 10 output for 'DETECTED: Tables are TRANSPOSED' message\")\n",
    "        print(f\"If you see this warning, the BIOM tables may still be in wrong orientation!\")\n",
    "        raise ValueError(f\"Unreasonable sample count: {total_samples:,}. Check Cell 10 output.\")\n",
    "    elif total_samples < 50:\n",
    "        print(f\"\\n     WARNING: Only {total_samples:,} samples detected.\")\n",
    "        print(f\"This seems low. Expected ~400 samples for disease study.\")\n",
    "        print(f\"Check that sample filtering in Cell 10 found matching samples.\")\n",
    "    else:\n",
    "        print(f\" Sample count looks reasonable for {DISEASE} analysis\")\n",
    "\n",
    "    edge_count = G.number_of_edges()\n",
    "    est_time_per_sample = 0.02 + (edge_count * 0.000005)\n",
    "    est_total_time_minutes = (est_time_per_sample * total_samples) / 60\n",
    "\n",
    "    print(f\"\\n  Performance Estimates:\")\n",
    "    print(f\"Est. time per sample: ~{est_time_per_sample:.2f} seconds\")\n",
    "    print(f\"Est. total time: ~{est_total_time_minutes:.1f} minutes ({est_total_time_minutes/60:.1f} hours)\")\n",
    "\n",
    "    if total_samples > 1000000:\n",
    "        print(f\" MASSIVE DATASET ALERT: {total_samples:,} samples detected!\")\n",
    "        print(f\"This will take approximately {est_total_time_minutes/60:.1f} hours\")\n",
    "        print(f\"Consider processing in batches or subsampling\")\n",
    "    elif est_total_time_minutes > 1440:\n",
    "        print(f\" ALERT: Processing will take > 24 hours!\")\n",
    "        print(f\"STRONGLY recommend batch processing or subsampling\")\n",
    "    elif est_total_time_minutes > 60:\n",
    "        print(f\"  WARNING: Processing may take > 1 hour\")\n",
    "        print(f\"Consider reducing base graph size further\")\n",
    "    else:\n",
    "        print(f\" Processing time looks reasonable\")\n",
    "\n",
    "    est_memory_per_graph_mb = (G.number_of_nodes() * 100 + G.number_of_edges() * 50) / 1024 / 1024\n",
    "    est_total_memory_mb = est_memory_per_graph_mb * total_samples\n",
    "\n",
    "    print(f\"\\n Memory Estimates:\")\n",
    "    print(f\"Per graph: ~{est_memory_per_graph_mb:.1f} MB\")\n",
    "    print(f\"Total (all graphs): ~{est_total_memory_mb:.1f} MB ({est_total_memory_mb/1024:.2f} GB)\")\n",
    "\n",
    "    if est_total_memory_mb > 8000:\n",
    "        print(f\"  WARNING: High memory usage expected (>{est_total_memory_mb/1024:.1f} GB)\")\n",
    "\n",
    "    print(f\"\\n Feature Mapping:\")\n",
    "    print(f\"Sequence mappings: {len(seq_to_ids):,}\")\n",
    "    print(f\"{DISEASE} features: {len(df_CASE.index):,}\")\n",
    "    print(f\"Control features: {len(df_CONTROL.index):,}\")\n",
    "    print(f\"Mapping coverage: {len(seq_to_ids) / len(set(df_CASE.index) | set(df_CONTROL.index)) * 100:.1f}%\")\n",
    "\n",
    "    print(f\"\\n Running garbage collection...\")\n",
    "    gc.collect()\n",
    "    print(f\" Memory cleaned\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "    if total_samples > 1000000:\n",
    "        print(\"MASSIVE DATASET DETECTED - RECOMMENDATIONS:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\n  Processing {total_samples:,} samples will take ~{est_total_time_minutes/60:.1f} hours\")\n",
    "        print(f\"Memory requirement: ~{est_total_memory_mb/1024:.1f} GB\")\n",
    "\n",
    "        print(f\"\\n RECOMMENDED SOLUTIONS:\")\n",
    "        print(f\"1. BATCH PROCESSING:\")\n",
    "        print(f\"   - Process in chunks of 10,000-50,000 samples\")\n",
    "        print(f\"   - Save results after each batch\")\n",
    "        print(f\"   - Use multiple workers if available\")\n",
    "\n",
    "        print(f\"\\n   2. SUBSAMPLING:\")\n",
    "        print(f\"   - Use stratified sampling (keep class balance)\")\n",
    "        print(f\"   - Start with 100,000 samples for testing\")\n",
    "        print(f\"   - Scale up if results look good\")\n",
    "\n",
    "        print(f\"\\n   3. GRAPH SIZE REDUCTION:\")\n",
    "        print(f\"   - Further reduce max_edges to 500 or less\")\n",
    "        print(f\"   - Increase distance_threshold to 1.02\")\n",
    "        print(f\"   - Consider feature selection first\")\n",
    "\n",
    "        print(f\"\\n   4. INFRASTRUCTURE:\")\n",
    "        print(f\"   - Use high-memory server (32GB+ RAM)\")\n",
    "        print(f\"   - Consider distributed processing\")\n",
    "        print(f\"   - Enable parallel processing if stable\")\n",
    "\n",
    "        print(f\"\\n QUICK FIX - Run this to reduce graph size:\")\n",
    "        print(f\"In Cell 15, change:\")\n",
    "        print(f\"max_edges = min(500, G.number_of_nodes())  # Even smaller\")\n",
    "        print(f\"distance_threshold = median_weight * 1.02  # Very tight\")\n",
    "\n",
    "        print(f\"\\n Do you want to continue with current settings?\")\n",
    "        print(f\"This will process ALL {total_samples:,} samples!\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    else:\n",
    "        print(\"Pre-flight check complete. Ready to process samples.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7695f61",
   "metadata": {},
   "source": [
    "## Sample-Specific Graph Construction\n",
    "\n",
    "### Build Individual Graphs for Each Sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f714597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Skipping - using existing graphs\")\n",
    "else:\n",
    "    if checkpoint_data['step'] < 4 or len(checkpoint_data.get('graphs', [])) == 0:\n",
    "        print(\"Constructing sample-specific graphs...\")\n",
    "        print(f\" Optimizations: Batch checkpointing + Better progress tracking\")\n",
    "\n",
    "        GRAPHS = []\n",
    "        LABELS = []\n",
    "        GRAPH_METADATA = []\n",
    "\n",
    "        CHECKPOINT_INTERVAL = 50\n",
    "\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        samples_processed = 0\n",
    "\n",
    "                                                                                   \n",
    "                                                                                   \n",
    "        target_config = {}\n",
    "        if 'EXPERIMENT_CONFIG' in globals() and EXPERIMENT_CONFIG:\n",
    "            target_config = EXPERIMENT_CONFIG.get('data_extraction', {}).get('target', {})\n",
    "        \n",
    "        target_type = target_config.get('type', 'binary')\n",
    "        num_classes = target_config.get('num_classes', 2)\n",
    "        \n",
    "                                                             \n",
    "        if target_type == 'ibd_subtype' and DISEASE != 'IBD':\n",
    "            print(f\"  ⚠️ Warning: ibd_subtype target is only valid for IBD disease, falling back to binary\")\n",
    "            target_type = 'binary'\n",
    "            num_classes = 2\n",
    "        \n",
    "        print(f\"\\n Label configuration:\")\n",
    "        print(f\"  Target type: {target_type}\")\n",
    "        print(f\"  Num classes: {num_classes}\")\n",
    "        \n",
    "                                                                              \n",
    "        sample_to_subtype = {}\n",
    "        sample_to_age = {}\n",
    "        \n",
    "        if target_type in ['ibd_subtype', 'age_category']:\n",
    "                                \n",
    "            case_metadata_path = output_dir / \"metadata\" / f\"AGP_{DISEASE}_cases_metadata.txt\"\n",
    "            if case_metadata_path.exists():\n",
    "                case_meta_df = pd.read_csv(case_metadata_path, sep='\\t')\n",
    "                sample_col = '#SampleID' if '#SampleID' in case_meta_df.columns else case_meta_df.columns[0]\n",
    "                \n",
    "                                     \n",
    "                if 'ibd_diagnosis_refined' in case_meta_df.columns:\n",
    "                    for _, row in case_meta_df.iterrows():\n",
    "                        subtype = row.get('ibd_diagnosis_refined', '')\n",
    "                        if \"Crohn\" in str(subtype):\n",
    "                            sample_to_subtype[row[sample_col]] = 1               \n",
    "                        elif \"Ulcerative\" in str(subtype):\n",
    "                            sample_to_subtype[row[sample_col]] = 2          \n",
    "                        else:\n",
    "                            sample_to_subtype[row[sample_col]] = 1                                    \n",
    "                \n",
    "                                      \n",
    "                if 'age_cat' in case_meta_df.columns:\n",
    "                    age_mapping = {'20s': 0, '30s': 1, '40s': 2, '50s': 3, '60s': 4, '70+': 4}\n",
    "                    for _, row in case_meta_df.iterrows():\n",
    "                        age = str(row.get('age_cat', ''))\n",
    "                        sample_to_age[row[sample_col]] = age_mapping.get(age, 2)                  \n",
    "                \n",
    "                print(f\"  Loaded {len(case_meta_df)} case metadata records\")\n",
    "            \n",
    "                                                             \n",
    "            ctrl_metadata_path = output_dir / \"metadata\" / f\"AGP_{DISEASE}_controls_metadata.txt\"\n",
    "            if ctrl_metadata_path.exists():\n",
    "                ctrl_meta_df = pd.read_csv(ctrl_metadata_path, sep='\\t')\n",
    "                sample_col = '#SampleID' if '#SampleID' in ctrl_meta_df.columns else ctrl_meta_df.columns[0]\n",
    "                \n",
    "                if 'age_cat' in ctrl_meta_df.columns:\n",
    "                    age_mapping = {'20s': 0, '30s': 1, '40s': 2, '50s': 3, '60s': 4, '70+': 4}\n",
    "                    for _, row in ctrl_meta_df.iterrows():\n",
    "                        age = str(row.get('age_cat', ''))\n",
    "                        sample_to_age[row[sample_col]] = age_mapping.get(age, 2)\n",
    "                \n",
    "                print(f\"  Loaded {len(ctrl_meta_df)} control metadata records\")\n",
    "\n",
    "        def get_sample_label(sample_id, is_case):\n",
    "            \"\"\"Get label based on target type and sample info.\"\"\"\n",
    "            if target_type == 'binary':\n",
    "                return 1 if is_case else 0\n",
    "            elif target_type == 'ibd_subtype':\n",
    "                if is_case:\n",
    "                    return sample_to_subtype.get(sample_id, 1)                   \n",
    "                else:\n",
    "                    return 0               \n",
    "            elif target_type == 'age_category':\n",
    "                return sample_to_age.get(sample_id, 2)                         \n",
    "            else:\n",
    "                return 1 if is_case else 0                      \n",
    "\n",
    "        print(f\"\\n Processing {DISEASE} cases...\")\n",
    "        case_samples = list(df_CASE.columns)\n",
    "        case_features = list(df_CASE.index)\n",
    "\n",
    "        print(f\"Total {DISEASE} samples: {len(case_samples)}\")\n",
    "\n",
    "        for i, sample in enumerate(tqdm(case_samples, desc=f\"{DISEASE} cases\")):\n",
    "            abundance_dict = {}\n",
    "            for j, feature in enumerate(case_features):\n",
    "                if feature in seq_to_ids:\n",
    "                    phylogenetic_id = seq_to_ids[feature]\n",
    "                    abundance_dict[phylogenetic_id] = df_CASE.loc[feature, sample]\n",
    "\n",
    "            try:\n",
    "                g_sample = adjust_graph_to_abundance(G, abundance_dict, GRAPH_PARAMS)\n",
    "\n",
    "                if GRAPH_PARAMS.get(\"randomize_edges\", False):\n",
    "                    from src.graph_utils import randomize_edges\n",
    "                    g_sample = randomize_edges(\n",
    "                        g_sample, \n",
    "                        preserve_degree=GRAPH_PARAMS.get(\"preserve_degree\", True),\n",
    "                        seed=RANDOM_SEED + i\n",
    "                    )\n",
    "\n",
    "                                                                                         \n",
    "                                                                       \n",
    "\n",
    "                if (g_sample.number_of_nodes() >= GRAPH_PARAMS[\"min_nodes\"] and \n",
    "                    g_sample.number_of_edges() >= GRAPH_PARAMS[\"min_edges\"]):\n",
    "\n",
    "                                                                           \n",
    "                    sample_label = get_sample_label(sample, is_case=True)\n",
    "                    g_sample.graph['label'] = sample_label\n",
    "                    g_sample.graph['sample_id'] = sample\n",
    "                    \n",
    "                    GRAPHS.append(g_sample)\n",
    "                    LABELS.append(sample_label)\n",
    "                    GRAPH_METADATA.append({\n",
    "                        \"sample_id\": sample,\n",
    "                        \"group\": DISEASE.upper(),\n",
    "                        \"label\": sample_label,\n",
    "                        \"n_nodes\": g_sample.number_of_nodes(),\n",
    "                        \"n_edges\": g_sample.number_of_edges(),\n",
    "                        \"avg_degree\": sum(dict(g_sample.degree()).values()) / g_sample.number_of_nodes() if g_sample.number_of_nodes() > 0 else 0,\n",
    "                        \"is_connected\": nx.is_connected(g_sample)\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\" Skipping {sample}: insufficient size ({g_sample.number_of_nodes()} nodes, {g_sample.number_of_edges()} edges)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" Error processing {sample}: {e}\")\n",
    "\n",
    "            samples_processed += 1\n",
    "\n",
    "            if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = samples_processed / elapsed\n",
    "                remaining = len(case_samples) - (i + 1)\n",
    "                eta_minutes = (remaining / rate) / 60 if rate > 0 else 0\n",
    "\n",
    "                checkpoint_data['graphs'] = GRAPHS\n",
    "                checkpoint_data['labels'] = LABELS\n",
    "                checkpoint_data['metadata'] = GRAPH_METADATA\n",
    "                save_checkpoint(checkpoint_data, CHECKPOINT_FILE)\n",
    "                print(f\" Checkpoint: {i+1}/{len(case_samples)} samples | Rate: {rate:.1f} samples/sec | ETA: {eta_minutes:.1f} min\")\n",
    "\n",
    "        print(f\" {DISEASE} cases processed: {len([l for l in LABELS if l == 1])} valid graphs\")\n",
    "\n",
    "        print(\"\\n Processing controls...\")\n",
    "        control_samples = list(df_CONTROL.columns)\n",
    "        control_features = list(df_CONTROL.index)\n",
    "\n",
    "        print(f\"Total control samples: {len(control_samples)}\")\n",
    "\n",
    "        for i, sample in enumerate(tqdm(control_samples, desc=\"Controls\")):\n",
    "            abundance_dict = {}\n",
    "            for j, feature in enumerate(control_features):\n",
    "                if feature in seq_to_ids:\n",
    "                    phylogenetic_id = seq_to_ids[feature]\n",
    "                    abundance_dict[phylogenetic_id] = df_CONTROL.loc[feature, sample]\n",
    "\n",
    "            try:\n",
    "                g_sample = adjust_graph_to_abundance(G, abundance_dict, GRAPH_PARAMS)\n",
    "\n",
    "                if GRAPH_PARAMS.get(\"randomize_edges\", False):\n",
    "                    from src.graph_utils import randomize_edges\n",
    "                    g_sample = randomize_edges(\n",
    "                        g_sample, \n",
    "                        preserve_degree=GRAPH_PARAMS.get(\"preserve_degree\", True),\n",
    "                        seed=RANDOM_SEED + len(case_samples) + i\n",
    "                    )\n",
    "\n",
    "                                                                                         \n",
    "                                                                       \n",
    "\n",
    "                if (g_sample.number_of_nodes() >= GRAPH_PARAMS[\"min_nodes\"] and \n",
    "                    g_sample.number_of_edges() >= GRAPH_PARAMS[\"min_edges\"]):\n",
    "\n",
    "                                                                           \n",
    "                    sample_label = get_sample_label(sample, is_case=False)\n",
    "                    g_sample.graph['label'] = sample_label\n",
    "                    g_sample.graph['sample_id'] = sample\n",
    "                    \n",
    "                    GRAPHS.append(g_sample)\n",
    "                    LABELS.append(sample_label)\n",
    "                    GRAPH_METADATA.append({\n",
    "                        \"sample_id\": sample,\n",
    "                        \"group\": \"Control\",\n",
    "                        \"label\": sample_label,\n",
    "                        \"n_nodes\": g_sample.number_of_nodes(),\n",
    "                        \"n_edges\": g_sample.number_of_edges(),\n",
    "                        \"avg_degree\": sum(dict(g_sample.degree()).values()) / g_sample.number_of_nodes() if g_sample.number_of_nodes() > 0 else 0,\n",
    "                        \"is_connected\": nx.is_connected(g_sample)\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\" Skipping {sample}: insufficient size ({g_sample.number_of_nodes()} nodes, {g_sample.number_of_edges()} edges)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" Error processing {sample}: {e}\")\n",
    "\n",
    "            samples_processed += 1\n",
    "\n",
    "            if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = samples_processed / elapsed\n",
    "                remaining = len(control_samples) - (i + 1)\n",
    "                eta_minutes = (remaining / rate) / 60 if rate > 0 else 0\n",
    "\n",
    "                checkpoint_data['graphs'] = GRAPHS\n",
    "                checkpoint_data['labels'] = LABELS\n",
    "                checkpoint_data['metadata'] = GRAPH_METADATA\n",
    "                save_checkpoint(checkpoint_data, CHECKPOINT_FILE)\n",
    "                print(f\" Checkpoint: {i+1}/{len(control_samples)} samples | Rate: {rate:.1f} samples/sec | ETA: {eta_minutes:.1f} min\")\n",
    "\n",
    "        print(f\" Controls processed: {len([l for l in LABELS if l == 0])} valid graphs\")\n",
    "\n",
    "        total_elapsed = time.time() - start_time\n",
    "        print(f\"\\n Graph construction summary:\")\n",
    "        print(f\"Total graphs: {len(GRAPHS)}\")\n",
    "        print(f\"{DISEASE} cases: {len([l for l in LABELS if l == 1])}\")\n",
    "        print(f\"Control graphs: {len([l for l in LABELS if l == 0])}\")\n",
    "        print(f\"Success rate: {len(GRAPHS)/(len(case_samples) + len(control_samples))*100:.1f}%\")\n",
    "        print(f\"Total time: {total_elapsed/60:.1f} minutes\")\n",
    "        print(f\"Average rate: {samples_processed/total_elapsed:.2f} samples/second\")\n",
    "\n",
    "        checkpoint_data['graphs'] = GRAPHS\n",
    "        checkpoint_data['labels'] = LABELS\n",
    "        checkpoint_data['metadata'] = GRAPH_METADATA\n",
    "        checkpoint_data['step'] = 4\n",
    "        save_checkpoint(checkpoint_data, CHECKPOINT_FILE)\n",
    "        print(f\" Final checkpoint saved\")\n",
    "\n",
    "    else:\n",
    "        print(\"Sample-specific graphs already constructed from checkpoint\")\n",
    "        GRAPHS = checkpoint_data['graphs']\n",
    "        LABELS = checkpoint_data['labels']\n",
    "        GRAPH_METADATA = checkpoint_data['metadata']\n",
    "        print(f\"Total graphs: {len(GRAPHS)}\")\n",
    "        print(f\"{DISEASE} cases: {len([l for l in LABELS if l == 1])}\")\n",
    "        print(f\"Control graphs: {len([l for l in LABELS if l == 0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34cc346",
   "metadata": {},
   "source": [
    "## Post-Processing and Memory Cleanup\n",
    "\n",
    "### Memory Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Skipping - using existing graphs\")\n",
    "else:\n",
    "    import gc\n",
    "    import sys\n",
    "\n",
    "    print(\"Cleaning up memory...\")\n",
    "\n",
    "    try:\n",
    "        import psutil\n",
    "        import os\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_before_mb = process.memory_info().rss / 1024 / 1024\n",
    "        print(f\"Memory before cleanup: {mem_before_mb:.1f} MB\")\n",
    "        has_psutil = True\n",
    "    except ImportError:\n",
    "        has_psutil = False\n",
    "        print(f\"(Install psutil for memory monitoring)\")\n",
    "\n",
    "    if 'dist_mat' in locals():\n",
    "        print(f\"Clearing distance matrix...\")\n",
    "        del dist_mat\n",
    "\n",
    "    if 'df_CASE' in locals() and 'df_CONTROL' in locals():\n",
    "        print(f\"Abundance tables will be kept for reference\")\n",
    "\n",
    "    for var_name in ['abundance_dict', 'g_sample', 'g_copy']:\n",
    "        if var_name in locals():\n",
    "            exec(f\"del {var_name}\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    if has_psutil:\n",
    "        mem_after_mb = process.memory_info().rss / 1024 / 1024\n",
    "        mem_freed_mb = mem_before_mb - mem_after_mb\n",
    "        print(f\"Memory after cleanup: {mem_after_mb:.1f} MB\")\n",
    "        if mem_freed_mb > 0:\n",
    "            print(f\"Freed: {mem_freed_mb:.1f} MB\")\n",
    "\n",
    "    print(f\" Memory cleanup complete\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23183800",
   "metadata": {},
   "source": [
    "### Graph Validation and Quality Control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c052b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Skipping graph validation - using existing graphs\")\n",
    "else:\n",
    "    print(\"Validating and cleaning graphs...\")\n",
    "\n",
    "    def ensure_node_weight_scalar(G_in):\n",
    "        \"\"\"Convert node weights from lists to scalars for compatibility.\"\"\"\n",
    "        for node, data in G_in.nodes(data=True):\n",
    "            weight = data.get(\"weight\", 0.0)\n",
    "            while isinstance(weight, list):\n",
    "                if len(weight) == 0:\n",
    "                    weight = 0.0\n",
    "                    break\n",
    "                elif len(weight) == 1:\n",
    "                    weight = weight[0]\n",
    "                else:\n",
    "                    weight = weight[0]\n",
    "                    break\n",
    "            data[\"weight\"] = float(weight)\n",
    "        return G_in\n",
    "\n",
    "    if len(GRAPHS) > 0:\n",
    "        print(f\"Cleaning {len(GRAPHS):,} graphs...\")\n",
    "        GRAPHS = [ensure_node_weight_scalar(g) for g in GRAPHS]\n",
    "\n",
    "        print(f\"Validating graph properties...\")\n",
    "        valid_graphs = []\n",
    "        valid_labels = []\n",
    "        valid_metadata = []\n",
    "        invalid_count = 0\n",
    "\n",
    "        for i, (graph, label, metadata) in enumerate(zip(GRAPHS, LABELS, GRAPH_METADATA)):\n",
    "            is_valid = (\n",
    "                graph.number_of_nodes() >= GRAPH_PARAMS[\"min_nodes\"] and \n",
    "                graph.number_of_edges() >= GRAPH_PARAMS[\"min_edges\"]\n",
    "            )\n",
    "\n",
    "            if GRAPH_PARAMS.get(\"connectivity_check\", False):\n",
    "                is_valid = is_valid and nx.is_connected(graph)\n",
    "\n",
    "            if is_valid:\n",
    "                valid_graphs.append(graph)\n",
    "                valid_labels.append(label)\n",
    "                valid_metadata.append(metadata)\n",
    "            else:\n",
    "                invalid_count += 1\n",
    "                if invalid_count <= 5:\n",
    "                    print(f\" Removing invalid graph {i}: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
    "\n",
    "        if invalid_count > 5:\n",
    "            print(f\" ... and {invalid_count - 5} more invalid graphs\")\n",
    "\n",
    "        GRAPHS = valid_graphs\n",
    "        LABELS = valid_labels\n",
    "        GRAPH_METADATA = valid_metadata\n",
    "\n",
    "        print(f\" Valid graphs: {len(GRAPHS):,} (removed {invalid_count} invalid)\")\n",
    "\n",
    "        if len(GRAPHS) > 0:\n",
    "            node_counts = [g.number_of_nodes() for g in GRAPHS]\n",
    "            edge_counts = [g.number_of_edges() for g in GRAPHS]\n",
    "\n",
    "            print(f\"\\n    Graph statistics:\")\n",
    "            print(f\"   Nodes - Mean: {np.mean(node_counts):.1f}, Median: {np.median(node_counts):.1f}, Range: {min(node_counts)}-{max(node_counts)}\")\n",
    "            print(f\"   Edges - Mean: {np.mean(edge_counts):.1f}, Median: {np.median(edge_counts):.1f}, Range: {min(edge_counts)}-{max(edge_counts)}\")\n",
    "\n",
    "            case_count = sum(LABELS)\n",
    "            control_count = len(LABELS) - case_count\n",
    "            balance_ratio = min(case_count, control_count) / max(case_count, control_count) if max(case_count, control_count) > 0 else 0\n",
    "\n",
    "            print(f\"\\n     Class balance:\")\n",
    "            print(f\"   {DISEASE} cases: {case_count:,} ({case_count/len(LABELS)*100:.1f}%)\")\n",
    "            print(f\"   Controls: {control_count:,} ({control_count/len(LABELS)*100:.1f}%)\")\n",
    "            print(f\"   Balance ratio: {balance_ratio:.2f} {'' if balance_ratio > 0.7 else ''}\")\n",
    "\n",
    "            if len(GRAPHS) > 0:\n",
    "                example_graph = GRAPHS[0]\n",
    "                print(f\"\\n    Example graph (first sample):\")\n",
    "                print(f\"   Nodes: {example_graph.number_of_nodes()}\")\n",
    "                print(f\"   Edges: {example_graph.number_of_edges()}\")\n",
    "                print(f\"   Density: {nx.density(example_graph):.4f}\")\n",
    "                print(f\"   Sample node weights: {[round(w, 4) for n, w in list(example_graph.nodes(data='weight'))[:3]]}...\")\n",
    "\n",
    "                if example_graph.number_of_edges() == 0:\n",
    "                    print(f\"WARNING: Example graph has no edges!\")\n",
    "                if example_graph.number_of_nodes() < 10:\n",
    "                    print(f\"WARNING: Example graph has very few nodes!\")\n",
    "    else:\n",
    "        print(f\" No graphs to validate!\")\n",
    "\n",
    "    print(\"\\n    Graph validation complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e9a6c",
   "metadata": {},
   "source": [
    "## Graph Visualization and Analysis\n",
    "\n",
    "### Visualize Example Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c2bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"Skipping graph visualization - using existing graphs\")\n",
    "else:\n",
    "    def plot_graph_example(graph, title=\"Graph Example\", max_nodes=VIZ_PARAMS[\"max_nodes_to_plot\"]):\n",
    "        \"\"\"Plot an example graph with node sizes based on weights.\"\"\"\n",
    "        if graph.number_of_nodes() == 0:\n",
    "            print(f\" Cannot plot empty graph: {title}\")\n",
    "            return\n",
    "\n",
    "        if graph.number_of_nodes() > max_nodes:\n",
    "            nodes_to_plot = list(graph.nodes())[:max_nodes]\n",
    "            subgraph = graph.subgraph(nodes_to_plot)\n",
    "            title += f\" (showing {max_nodes}/{graph.number_of_nodes()} nodes)\"\n",
    "        else:\n",
    "            subgraph = graph\n",
    "\n",
    "        plt.figure(figsize=VIZ_PARAMS[\"figsize\"], dpi=VIZ_PARAMS[\"dpi\"])\n",
    "\n",
    "        pos = nx.spring_layout(subgraph, k=1, iterations=50)\n",
    "\n",
    "        node_weights = [subgraph.nodes[node].get('weight', 0.1) for node in subgraph.nodes()]\n",
    "        node_sizes = [max(10, w * VIZ_PARAMS[\"node_size_scale\"]) for w in node_weights]\n",
    "\n",
    "        edge_weights = [subgraph.edges[edge].get('weight', 0.1) for edge in subgraph.edges()]\n",
    "\n",
    "        nx.draw(subgraph, pos, \n",
    "                node_size=node_sizes,\n",
    "                node_color='lightblue',\n",
    "                edge_color='gray',\n",
    "                width=VIZ_PARAMS[\"edge_width_scale\"],\n",
    "                with_labels=False,\n",
    "                alpha=0.7)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "\n",
    "        if VIZ_PARAMS[\"save_plots\"]:\n",
    "            plot_path = graphs_output_dir / \"graph_examples\" / f\"{title.replace(' ', '_').replace('(', '').replace(')', '')}.png\"\n",
    "            plt.savefig(plot_path, dpi=VIZ_PARAMS[\"dpi\"], bbox_inches='tight')\n",
    "            print(f\" Saved plot: {plot_path}\")\n",
    "\n",
    "        if VIZ_PARAMS[\"show_plots\"]:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    def plot_graph_statistics():\n",
    "        \"\"\"Plot comprehensive graph statistics.\"\"\"\n",
    "        if len(GRAPHS) == 0:\n",
    "            print(f\" No graphs to analyze\")\n",
    "            return\n",
    "\n",
    "        metadata_df = pd.DataFrame(GRAPH_METADATA)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Graph Construction Statistics', fontsize=16)\n",
    "\n",
    "        axes[0, 0].hist(metadata_df['n_nodes'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_xlabel('Number of Nodes')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Distribution of Node Counts')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[0, 1].hist(metadata_df['n_edges'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[0, 1].set_xlabel('Number of Edges')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title('Distribution of Edge Counts')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        group_stats = metadata_df.groupby('group')['avg_degree'].agg(['mean', 'std']).reset_index()\n",
    "        axes[1, 0].bar(group_stats['group'], group_stats['mean'], \n",
    "                       yerr=group_stats['std'], capsize=5, alpha=0.7, color=['red', 'blue'])\n",
    "        axes[1, 0].set_ylabel('Average Degree')\n",
    "        axes[1, 0].set_title('Average Degree by Group')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        class_counts = metadata_df['group'].value_counts()\n",
    "        axes[1, 1].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', \n",
    "                       colors=['red', 'blue'], wedgeprops={'alpha': 0.7})\n",
    "        axes[1, 1].set_title('Class Balance')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if VIZ_PARAMS[\"save_plots\"]:\n",
    "            plot_path = visualizations_dir / \"statistics\" / \"graph_statistics.png\"\n",
    "            plt.savefig(plot_path, dpi=VIZ_PARAMS[\"dpi\"], bbox_inches='tight')\n",
    "            print(f\" Saved statistics plot: {plot_path}\")\n",
    "\n",
    "        if VIZ_PARAMS[\"show_plots\"]:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    if len(GRAPHS) > 0:\n",
    "        print(\"Generating graph visualizations...\")\n",
    "\n",
    "        case_graphs = [g for g, l in zip(GRAPHS, LABELS) if l == 1]\n",
    "        control_graphs = [g for g, l in zip(GRAPHS, LABELS) if l == 0]\n",
    "\n",
    "        if len(case_graphs) > 0:\n",
    "            print(f\" Plotting {DISEASE} case example graph...\")\n",
    "            plot_graph_example(case_graphs[0], f\"{DISEASE} Case Example Graph\")\n",
    "\n",
    "        if len(control_graphs) > 0:\n",
    "            print(f\" Plotting Control example graph...\")\n",
    "            plot_graph_example(control_graphs[0], \"Control Example Graph\")\n",
    "\n",
    "        print(f\" Generating statistics plots...\")\n",
    "        plot_graph_statistics()\n",
    "\n",
    "        print(f\" Visualizations complete\")\n",
    "    else:\n",
    "        print(f\" No graphs available for visualization\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71552fd",
   "metadata": {},
   "source": [
    "## Output Generation and Pipeline Summary\n",
    "\n",
    "### Save Output Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93519c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP_GRAPH_CONSTRUCTION:\n",
    "    print(\"=\"*60)\n",
    "    print(\"USING EXISTING GRAPHS - NO SAVE NEEDED\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Graphs already exist at: {OUT_GRAPHS_PKL}\")\n",
    "    print(f\"Labels already exist at: {OUT_LABELS_NPY}\")\n",
    "    print(f\"Number of graphs: {len(GRAPHS_PREPARED)}\")\n",
    "    print(f\"Number of labels: {len(LABELS)}\")\n",
    "    GRAPHS = GRAPHS_PREPARED\n",
    "else:\n",
    "    print(\"Saving output files...\")\n",
    "\n",
    "    if len(GRAPHS) > 0:\n",
    "        print(f\"Saving {len(GRAPHS)} graphs to {OUT_GRAPHS_PKL}\")\n",
    "        with open(OUT_GRAPHS_PKL, \"wb\") as f:\n",
    "            pickle.dump(GRAPHS, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        print(f\"Saving {len(LABELS)} labels to {OUT_LABELS_NPY}\")\n",
    "        np.save(OUT_LABELS_NPY, np.array(LABELS, dtype=np.int64))\n",
    "        \n",
    "        print(f\"Saving metadata to {OUT_METADATA_CSV}\")\n",
    "        metadata_df = pd.DataFrame(GRAPH_METADATA)\n",
    "        metadata_df.to_csv(OUT_METADATA_CSV, index=False)\n",
    "        \n",
    "        config_output = {\n",
    "            \"disease\": DISEASE,\n",
    "            \"graph_params\": GRAPH_PARAMS,\n",
    "            \"viz_params\": VIZ_PARAMS,\n",
    "            \"n_graphs\": len(GRAPHS),\n",
    "            \"n_cases\": sum(LABELS),\n",
    "            \"n_control\": len(LABELS) - sum(LABELS),\n",
    "            \"base_graph_nodes\": G.number_of_nodes(),\n",
    "            \"base_graph_edges\": G.number_of_edges(),\n",
    "            \"stockholm_available\": stockholm_available,\n",
    "            \"sequence_mappings\": len(seq_to_ids),\n",
    "            \"pipeline_version\": \"1.0\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"input_files\": {\n",
    "                \"distance_matrix\": str(PICKLE_FILE),\n",
    "                \"cases\": str(CASE_BIOM_TSV),\n",
    "                \"controls\": str(CONTROL_BIOM_TSV),\n",
    "                \"case_ids\": str(CASE_IDS_FILE),\n",
    "                \"control_ids\": str(CONTROL_IDS_FILE)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(OUT_CONFIG_JSON, \"w\") as f:\n",
    "            json.dump(config_output, f, indent=2)\n",
    "        \n",
    "        print(f\" All files saved successfully\")\n",
    "        \n",
    "        print(f\"\\n Final Summary:\")\n",
    "        print(f\"Disease: {DISEASE}\")\n",
    "        print(f\"Total graphs: {len(GRAPHS)}\")\n",
    "        print(f\"{DISEASE} graphs: {sum(LABELS)}\")\n",
    "        print(f\"Control graphs: {len(LABELS) - sum(LABELS)}\")\n",
    "        print(f\"Base graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "        print(f\"Sequence mappings: {len(seq_to_ids)}\")\n",
    "        \n",
    "        print(f\"\\n Output files:\")\n",
    "        print(f\"- Graphs: {OUT_GRAPHS_PKL}\")\n",
    "        print(f\"- Labels: {OUT_LABELS_NPY}\")\n",
    "        print(f\"- Metadata: {OUT_METADATA_CSV}\")\n",
    "        print(f\"- Config: {OUT_CONFIG_JSON}\")\n",
    "        print(f\"- Checkpoint: {CHECKPOINT_FILE}\")\n",
    "        \n",
    "        print(f\"\\n Visualization files:\")\n",
    "        print(f\"- Graph examples: {graphs_output_dir / 'graph_examples'}\")\n",
    "        print(f\"- Statistics: {visualizations_dir / 'statistics'}\")\n",
    "        \n",
    "        print(f\"\\n Next Steps:\")\n",
    "        print(f\"1. Use the graphs for machine learning analysis\")\n",
    "        print(f\"2. The metadata CSV contains graph statistics for each sample\")\n",
    "        print(f\"3. The configuration file contains all parameters for reproducibility\")\n",
    "        print(f\"4. Check the visualization files to understand graph structure\")\n",
    "        \n",
    "        checkpoint_data['step'] = 5\n",
    "        checkpoint_data['final_outputs'] = {\n",
    "            'graphs_file': str(OUT_GRAPHS_PKL),\n",
    "            'labels_file': str(OUT_LABELS_NPY),\n",
    "            'metadata_file': str(OUT_METADATA_CSV),\n",
    "            'config_file': str(OUT_CONFIG_JSON)\n",
    "        }\n",
    "        save_checkpoint(checkpoint_data, CHECKPOINT_FILE)\n",
    "        \n",
    "    else:\n",
    "        print(f\" No graphs to save!\")\n",
    "        print(f\"Check the input data and parameters\")\n",
    "\n",
    "print(f\"\\n Graph construction pipeline complete!\")\n",
    "print(f\" Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566aeb2-561a-430f-9457-c6678b2bf066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}